{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGnhK_xzWSWl"
   },
   "source": [
    "# 11411/611 -NLP (S23)\n",
    "# Asssignment 3: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rADZzHRUWqW4"
   },
   "source": [
    "Classifiers are helpful in distinguish texts from different categories. They are vey useful in numerous use cases.\n",
    "\n",
    "In this assignment you will build a Naive Bayes Classifier that will distinguish 6 different languages namely Hausa, Indonesisan, Manobo, Tagalog, Swahili and Nahuatl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSkIuUb2JvaQ"
   },
   "source": [
    "You are required to program a Naive Bayes Classifier in this HW.\n",
    "\n",
    "Submission Guidelines\n",
    "Deadline: \n",
    "\n",
    "Programming:\n",
    "\n",
    "This notebook contains helpful test cases and additional information about the programming part of the HW. However, you are only required to submit naivebayes.py on Gradescope.\n",
    "We recommended that you first code in the notebook and then copy the corresponding methods/classes to naivebayes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "id": "QHSGpROOYr3Z"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from operator import itemgetter\n",
    "from math import log\n",
    "from typing import DefaultDict\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "id": "DDg_1WnOawF8"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "\n",
    "    def extract_ngrams(self,x: str, n=2) -> \"list[str]\":\n",
    "        \"\"\"\n",
    "        Train a Naive-Bayes model\n",
    "\n",
    "        :param x: The document which needs to be decomposed into ngrams.\n",
    "        :para n: the order of ngrams.\n",
    "        :return: list of ngrams\n",
    "        \"\"\"\n",
    "        ###TODO###\n",
    "        #extract character ngrams\n",
    "        x = list(x)\n",
    "        n_grams = []\n",
    "        \n",
    "        if \"\\n\" in x:\n",
    "            n_grams.append(\"\\n\")\n",
    "        for i in range(0, len(x) - n, n):\n",
    "            n_grams.append(x[i : i + n])\n",
    "\n",
    "        return n_grams\n",
    "\n",
    "    def smoothed_log_likelihood(self, w: str, c: str, k: int, count: 'DefaultDict[str, Counter]', vocab: \"set[str]\") -> float:\n",
    "        \"\"\"\n",
    "        :param w: word in the vocab\n",
    "        :para c: class label\n",
    "        :param k: The value added to the numerator and denominator to smooth likelihoods\n",
    "        :para count: Dictionary containing the count of label c occuring with an ngram w\n",
    "        :param vocab: the vocabulary for the model\n",
    "        :type b: int\n",
    "        :return: the log likelihood value after smoothening\n",
    "        \"\"\"\n",
    "        ###TODO###\n",
    "        #apply smoothing\n",
    "        word_count = 0\n",
    "        vocabulary_count = {}\n",
    "\n",
    "        for doc in count[c]:\n",
    "            word_count += doc.count(w)\n",
    "            \n",
    "            for vocabulary in vocab:\n",
    "                if vocabulary != w:\n",
    "                    if vocabulary in vocabulary_count:\n",
    "                        vocabulary_count[vocabulary] += doc.count(vocabulary)\n",
    "                    else:\n",
    "                        vocabulary_count[vocabulary] = doc.count(vocabulary) + k\n",
    "        \n",
    "        \n",
    "        summation_of_vocab_count = 0\n",
    "\n",
    "        for vocab in vocabulary_count.keys():\n",
    "            summation_of_vocab_count += vocabulary_count[vocab]\n",
    "\n",
    "        log_likelihood = log((word_count + k)/(summation_of_vocab_count + word_count + k))\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def train_nb(self, docs: \"list[tuple[str, str]]\", k: int = 1, n: int = 2) -> \"tuple[dict[str, float], DefaultDict[str, DefaultDict[str, float]], set[str], set[str]]\":\n",
    "        ###TODO###\n",
    "        \"\"\"\n",
    "        Train a Naive-Bayes model\n",
    "\n",
    "        :param docs: The documents, each associated with a clas label (document, label)\n",
    "        :type docs: list[tuple[str, str]]\n",
    "        :param k: The value added to the numerator and denominator to smooth likelihoods\n",
    "        :type k: int\n",
    "        :para n: the order of ngrams.\n",
    "        :type b: int\n",
    "        :return: The log priors, log likelihoods, the classes, and the vocabulary for the model at a tuple\n",
    "        :rtype: tuple[dict[str, float], DefaultDict[str, DefaultDict[str, float]], set[str], set[str]]\n",
    "        \"\"\"\n",
    "        num_of_docs = len(docs)\n",
    "        log_priors = {}\n",
    "\n",
    "        for doc in docs:\n",
    "            doc_label = doc[0]\n",
    "\n",
    "            if doc_label in log_priors:\n",
    "                log_priors[doc_label] += 1\n",
    "            else:\n",
    "                log_priors[doc_label] = 1\n",
    "        \n",
    "        labels = log_priors.keys()\n",
    "\n",
    "        for label in labels:\n",
    "            log_priors[label] = log(log_priors[label]/num_of_docs)\n",
    "\n",
    "        vocabulary = {}\n",
    "        for doc in docs:\n",
    "            for vocab in self.extract_ngrams(doc[1]):\n",
    "                for v in vocab:\n",
    "                    vocabulary[v] = True\n",
    "\n",
    "        vocabulary = vocabulary.keys()\n",
    "        print(vocabulary)\n",
    "        big_doc = {}\n",
    "\n",
    "        for label in labels:\n",
    "            big_doc[label] = []\n",
    "\n",
    "        for doc in docs:\n",
    "            big_doc[doc[0]].append(doc[1])\n",
    "        \n",
    "        log_likelihoods = {}\n",
    "        for word in vocabulary:\n",
    "            for label in labels:\n",
    "                log_likelihood = self.smoothed_log_likelihood(word, label, k, big_doc, vocabulary)\n",
    "                \n",
    "                if word not in log_likelihoods:\n",
    "                    log_likelihoods[word] = {label:log_likelihood}\n",
    "                else:\n",
    "                    log_likelihoods[word][label] = log_likelihood\n",
    "            \n",
    "        return log_priors, log_likelihoods, labels, vocabulary\n",
    "\n",
    "    def classify(self, testdoc: str, log_prior: \"dict[str, float]\", log_likelihood: \"DefaultDict[str, DefaultDict[str, float]]\", classes: \"set[str]\", vocab: \"set[str]\", k: int=1, n: int=2) -> str:\n",
    "        ###TODO###\n",
    "        \"\"\"Given a trained NB model (log_prior, log_likelihood, classes, and vocab), returns the most likely label for the input document.\n",
    "\n",
    "        :param textdoc str: The test document.\n",
    "        :param log_prior dict[str, float]: The log priors of each category. Categories are keys and log priors are values.\n",
    "        :param log_likelihood DefaultDict[str, DefaultDict[str, float]]: The log likelihoods for each combination of word/ngram and class.\n",
    "        :param classes set[str]: The set of class labels (as strings).\n",
    "        :param vocab set[str]: The set of words/negrams in the vocabulary.\n",
    "        :param k int: the value added in smoothing.\n",
    "        \"param n int: the order of ngrams.\n",
    "        :return: The best label for `testdoc` in light of the model.\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "\n",
    "        ##TODO\n",
    "        # Extract a set of ngrams from `testdoc`\n",
    "        doc = self.extract_ngrams(testdoc)\n",
    "        ##TODO\n",
    "        # Initialize the sums for each class. These will be the \"scores\" based on which class will be assigned.\n",
    "        class_sum = {}\n",
    "        ##TODO\n",
    "        # Iterate over the classes, computing `class_sum` for each\n",
    "        for c in classes:\n",
    "            ##TODO\n",
    "            # Initialize `class_sum` with the log prior for the class\n",
    "            ##TODO\n",
    "            # Then add the likelihood for each in-vocabulary word/ngram in the document\n",
    "            class_sum[c] = 0\n",
    "\n",
    "            for vocab in self.extract_ngrams(testdoc):\n",
    "                for v in vocab:\n",
    "                    try:\n",
    "                        class_sum[c] += log_likelihood[v][c]\n",
    "                    except:    \n",
    "                        v_count = testdoc.count(v) + k\n",
    "                        v_complement_count = 0\n",
    "\n",
    "                        for v_complement in set(testdoc):\n",
    "                            v_complement_count += testdoc.count(v_complement) + k\n",
    "                        \n",
    "                        class_sum[c] += log(v_count/v_complement_count)\n",
    "        best_class = list(class_sum.keys())[0]\n",
    "        best_class_value = class_sum[best_class]\n",
    "\n",
    "        for a_class in class_sum.keys():\n",
    "            if best_class_value <= class_sum[a_class]:\n",
    "                best_class = a_class\n",
    "                best_class_value = class_sum[a_class]\n",
    "        \n",
    "        return best_class\n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "    def precision(self,tp: \"dict[str, int]\", fp: \"dict[str, int]\") -> float:\n",
    "        return tp / (tp + fp)\n",
    "\n",
    "    def recall(self,tp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
    "        return tp / (tp + fn)\n",
    "\n",
    "    def micro_precision(self, tp: \"dict[str, int]\", fp: \"dict[str, int]\") -> float:\n",
    "        tp_sum = sum(tp.values())\n",
    "        fp_sum = sum(fp.values())\n",
    "        return tp_sum / (tp_sum + fp_sum)\n",
    "\n",
    "    def micro_recall(self, tp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
    "        tp_sum = sum(tp.values())\n",
    "        fn_sum = sum(fn.values())\n",
    "        return tp_sum / (tp_sum + fn_sum)\n",
    "\n",
    "    def micro_f1(self, tp: \"dict[str, int]\", fp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
    "        mp = self.micro_precision(tp, fp)\n",
    "        mr = self.micro_recall(tp, fn)\n",
    "        return 2 * (mp * mr) / (mp + mr)\n",
    "\n",
    "    def macro_precision(self, tp: \"dict[str, int]\", fp: \"dict[str, int]\") -> float:\n",
    "        n = len(tp)\n",
    "        return (1 / n) * sum([self.precision(tp[c], fp[c]) for c in tp.keys()])\n",
    "\n",
    "    def macro_recall(self, tp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
    "        n = len(tp)\n",
    "        return (1 / n) * sum([self.recall(tp[c], fn[c]) for c in tp.keys()])\n",
    "\n",
    "    def macro_f1(self, tp: \"dict[str, int]\", fp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
    "        n = len(tp)\n",
    "        return 2 * (self.macro_precision(tp, fp) * self.macro_recall(tp, fn)) / (self.macro_precision(tp, fp) + self.macro_recall(tp, fn))\n",
    "\n",
    "    def evaluate(self, train: \"list[tuple[str, str]]\", eval: \"list[tuple[str, str]]\", n: int=2):\n",
    "        log_prior, log_likelihood, classes, vocab = self.train_nb(train, n = n)\n",
    "        # Initialize dictionaries for true positives, false positives, and false negatives\n",
    "        tp, fp, fn = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "        confusion = defaultdict(lambda: defaultdict(int))\n",
    "        for c_ref, doc in eval:\n",
    "            c_hyp = self.classify(doc, log_prior, log_likelihood, classes, vocab, n = n)\n",
    "            confusion[c_ref][c_hyp] += 1\n",
    "            if c_ref == c_hyp:\n",
    "                tp[c_ref] += 1\n",
    "            else:\n",
    "                fn[c_ref] += 1\n",
    "                fp[c_hyp] += 1\n",
    "\n",
    "        print(f'Macro-averaged precision:\\t{self.macro_precision(tp, fp)}')\n",
    "        print(f'Macro-averaged recall:\\t{self.macro_recall(tp, fn)}')\n",
    "        print(f'Macro-averaged F1:\\t{self.macro_f1(tp, fp, fn)}')\n",
    "        print(f'Micro-averaged precision:\\t{self.micro_precision(tp, fp)}')\n",
    "        print(f'Micro-averaged recall:\\t{self.micro_recall(tp, fn)}')\n",
    "        print(f'Micro-averaged F1:\\t{self.micro_f1(tp, fp, fn)}')\n",
    "\n",
    "        return self.macro_precision(tp, fp), self.macro_recall(tp, fn), self.macro_f1(tp, fp, fn), self.micro_precision(tp, fp), self.micro_recall(tp, fn), self.micro_f1(tp, fp, fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "id": "N-JR6By4Xh8D"
   },
   "outputs": [],
   "source": [
    "with open('train_small.tsv',  encoding='utf-8') as f:\n",
    "    train = [tuple(l.split('\\t')) for l in f]\n",
    "    \n",
    "with open('test_small.tsv',  encoding='utf-8') as f:\n",
    "    dev = [tuple(l.split('\\t')) for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Trained model\n",
    "nb_model = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['\\n', 'Y', 'o', 'u', ' ', 'a', 'r', 'e', 't', 'h', 'b', 's', 'd', 'p', 'n', 'N', 'l', 'g'])\n"
     ]
    }
   ],
   "source": [
    "log_prior, log_likelihood, classes, vocab = nb_model.train_nb(train, n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test log prior\n",
    "assert str(log_prior['1']) == '-0.5108256237659907'\n",
    "assert str(log_prior['0']) == '-0.916290731874155'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test vocab\n",
    "assert vocab == {'\\n', 'g', 'p', 'o', 'd', ' ', 'a', 'h', 'n', 's', 'b', 'u', 't', 'e', 'l', 'Y', 'r', 'N'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test log likelihood\n",
    "assert log_likelihood['g'] == {'1': -3.7376696182833684, '0': -4.23410650459726}\n",
    "assert log_likelihood['p'] == {'1': -3.044522437723423, '0': -3.1354942159291497}\n",
    "assert log_likelihood['Y'] == {'1': -3.332204510175204, '0': -3.1354942159291497}\n",
    "assert log_likelihood['N'] == {'1': -3.7376696182833684, '0': -4.23410650459726}\n",
    "assert log_likelihood['a'] == {'1': -2.4849066497880004, '0': -2.1546649629174235}\n",
    "assert log_likelihood['s'] == {'1': -3.332204510175204, '0': -3.1354942159291497}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "id": "rSz9MXmQXik9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['\\n', 'Y', 'o', 'u', ' ', 'a', 'r', 'e', 't', 'h', 'b', 's', 'd', 'p', 'n', 'N', 'l', 'g'])\n",
      "Macro-averaged precision:\t0.875\n",
      "Macro-averaged recall:\t0.75\n",
      "Macro-averaged F1:\t0.8076923076923077\n",
      "Micro-averaged precision:\t0.8\n",
      "Micro-averaged recall:\t0.8\n",
      "Micro-averaged F1:\t0.8000000000000002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Evaluation\n",
    "map, mar, maf, mp, mr, mf = nb_model.evaluate(train, dev, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "id": "k3cNih5xpbiH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875 0.75 0.8076923076923077 0.8 0.8 0.8000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(map, mar, maf, mp, mr, mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
