{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YId1X55ZPDWB"
      },
      "source": [
        "# Neural Machine Translation using Seq2Seq Models\n",
        "\n",
        "In HW06, we will be training our own machine translation system. We will use an LSTM encoder decoder model and train it with Pytorch.\n",
        "\n",
        "Make sure you are using a GPU. Go to Runtime -> Change Runtime Type -> GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQy46o98kAfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963322c1-ded5-4f0f-dd01-addb237aa84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (1.22.4)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (2022.6.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (0.8.10)\n",
            "Installing collected packages: sentencepiece, portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.7.0 sacrebleu-2.3.1 sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00jNulTjyG72"
      },
      "source": [
        "We will use the TED 2013 English-Mandarin parralel dataset. We have done some cleaning and preprocessing for you. We have also created the train, dev, and test splits. Upload the data from the handout and unzip the file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "x_UW3sA0XGBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2402f2f-97a9-448a-dc67-7948058cd162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data.zip\n",
            "   creating: data/\n",
            "  inflating: data/train.zh           \n",
            "  inflating: data/train.en           \n",
            "  inflating: data/valid.zh           \n",
            "  inflating: data/test.en            \n",
            "  inflating: data/valid.en           \n",
            "  inflating: data/test.zh            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu9aGskp58pS"
      },
      "source": [
        "Import the libraries we will use. After running the cell below, if your notebook is configured to run on GPU, you should see a `device(type='cuda')` output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2QfJZy3cygL",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f188fda6-8de9-4a47-e56a-79123135180e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import operator\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sacrebleu.metrics import BLEU, CHRF\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "device = torch.device(device)\n",
        "print(f\"Device being used: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFriOo4dyVC-"
      },
      "source": [
        "## Preprocessing and Tokenization\n",
        "\n",
        "We will use Sentencepiece to train BPE tokenization models for both the source and target languages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsL4_BeHga-s"
      },
      "source": [
        "Define a function to load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MoLDWprMAav"
      },
      "outputs": [],
      "source": [
        "def load_dataset(lang1, lang2, max_length=10, root=\"data\", split=\"train\"):\n",
        "    ipt1 = os.path.join(root, \".\".join([split, lang1]))\n",
        "    ipt2 = os.path.join(root, \".\".join([split, lang2]))\n",
        "    corpus = {lang1: [], lang2: []}\n",
        "    with open(ipt1) as f1, open(ipt2) as f2:\n",
        "        for sent1, sent2 in zip(f1, f2):\n",
        "            clean1 = sent1.strip()\n",
        "            clean2 = sent2.strip()\n",
        "            if len(clean1) == 0 or len(clean2) == 0:\n",
        "                continue\n",
        "            corpus[lang1].append(clean1)\n",
        "            corpus[lang2].append(clean2)\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t77jfic9MAav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a11f5d3-472d-403d-dfd3-9ee6c8e7384f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42708 42708\n"
          ]
        }
      ],
      "source": [
        "train_data = load_dataset('en', 'zh', split=\"train\")\n",
        "valid_data = load_dataset('en', 'zh', split=\"valid\")\n",
        "assert len(train_data['en']) == 42708, \"Should be 42708\"\n",
        "assert len(train_data['zh']) == 42708, \"Should be 42708\"\n",
        "assert len(valid_data['en']) == 5338, \"Should be 5338\"\n",
        "assert len(valid_data['zh']) == 5338, \"Should be 5338\"\n",
        "print(f\"{len(train_data['en'])=}, {len(train_data['zh'])=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIWufnOogsK7"
      },
      "source": [
        "Train the Sentencepiece models. Training the Mandarin model will take longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7T12yXjMAav"
      },
      "outputs": [],
      "source": [
        "spm.SentencePieceTrainer.train(\n",
        "    sentence_iterator=iter(train_data[\"en\"]),\n",
        "    model_prefix=\"en-zh.en\",\n",
        "    vocab_size=5000,\n",
        "    model_type=\"bpe\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgtt0saFMAav"
      },
      "outputs": [],
      "source": [
        "spm.SentencePieceTrainer.train(\n",
        "    sentence_iterator=iter(train_data[\"zh\"]),\n",
        "    model_prefix=\"en-zh.zh\",\n",
        "    vocab_size=5000,\n",
        "    model_type=\"bpe\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orQ6EWFOdjaC"
      },
      "source": [
        "### Checkpoint 1\n",
        "\n",
        "Test your newly trained Sentencepiece models on a snippet of the dataset. You can download the model files so that you don't have to redo the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmmXD5HOMAaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d47175e-7b34-4d4b-96d8-0fa2f48cb54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['▁What', '▁gives', '▁us', '▁the', '▁cou', 'rage', '?'], ['▁\"', 'C', 'ome', '▁on', '.', '▁You', '▁got', '▁a', '▁t', 'icket', '?\"'], ['▁P', 'ush', '▁it', '▁and', '▁it', '▁becomes', '▁house', '▁sha', 'ped', '.'], ['▁R', 'est', 'ing', '▁on', '▁the', '▁sa', 'uc', 'er', '▁were', '▁two', '▁pack', 'ets', '▁of', '▁su', 'g', 'ar', '.'], ['▁Now', ',', '▁here', '▁is', '▁K', 'ing', '▁C', 'n', 'ut', ',', '▁king', '▁a', '▁thousand', '▁years', '▁ago', '.'], ['▁And', '▁they', '▁say', ',', '▁\"', 'K', 'e', 'ep', '▁your', '▁laws', '▁off', '▁my', '▁body', '.\"'], ['▁The', '▁Bill', '▁G', 'ates', '▁song', '!'], ['▁You', \"'\", 're', '▁critical', '.'], ['▁Okay', ',', '▁so', '▁you', '▁smile', ',', '▁fr', 'own', 'ing', '.'], ['▁Because', '▁this', '▁didn', \"'\", 't', '▁want', '▁to', '▁be', '▁a', '▁surprise', '.']]\n"
          ]
        }
      ],
      "source": [
        "en_sp = spm.SentencePieceProcessor(model_file=\"en-zh.en.model\")\n",
        "assert en_sp.encode_as_pieces(train_data[\"en\"][0]) == ['▁What', '▁gives', '▁us', '▁the', '▁cou', 'rage', '?']\n",
        "print(en_sp.encode_as_pieces(train_data[\"en\"][:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTXXmUJ5MAaw",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4e786e-4833-4706-8c4f-8998ae10a1a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['▁什么', '给', '予', '了我', '这', '项', '勇', '气', '?'], ['▁“', '哦', '说', '吧', ',', '你', '吃', '罚', '单', '了', '?”'], ['▁', '推', '一下', ',', '它', '又', '成了', '屋', '形'], ['▁只', '见', '茶', '托', '上', '赫', '然', '躺', '着', '▁', '两', '小', '包', '糖', '。'], ['▁现在', '看到的是', '克', '努', '特', '王', ',', '1', '000', '年前', '的', '国', '王', '。'], ['▁他们说', '”', '不要', '给我', '这些', '法', '律', '。“'], ['▁比', '尔', '盖', '茨', '之', '歌', '!'], ['▁你', '有些', '怀', '疑', '.'], ['▁O', 'K', ',', '当', '你', '▁', '微', '笑', '和', '皱', '眉', '头', '时', '。'], ['▁因为我们', '不想', '这个项目', '让', '大家', '觉得', '突然', '。']]\n"
          ]
        }
      ],
      "source": [
        "zh_sp = spm.SentencePieceProcessor(model_file=\"en-zh.zh.model\")\n",
        "assert zh_sp.encode_as_pieces(train_data[\"zh\"][0]) == ['▁什么', '给', '予', '了我', '这', '项', '勇', '气', '?']\n",
        "print(zh_sp.encode_as_pieces(train_data[\"zh\"][:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHp1PYyUg9NM"
      },
      "source": [
        "## Model Definitions\n",
        "\n",
        "Now we need to define our models. In general, we need to inherit the `torch.nn.Module` class, define an `__init__` function where we create the layers in our model, and a `forward` function where we compute the outputs from the inputs.\n",
        "\n",
        "You can check [here](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) for a more detailed tutorial on building neural networks in Pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BVBkJVczD3D"
      },
      "source": [
        "For the encoder, we use a bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk_1ZuLWj5N9"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, emb_size, hidden_size, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # TO-DO: Create nn.Embedding layer with (input_size, emb_size)\n",
        "        self.embedding = None\n",
        "\n",
        "        # TO-DO: Create nn.LSTM layer with (emb_size, hidden_size)\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=# TO-DO,\n",
        "            hidden_size=# TO-DO,\n",
        "            num_layers=# TO-DO,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=# TO-DO,\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # TO-DO: Run the input through the embedding layer\n",
        "        embedded = None\n",
        "\n",
        "        # TO-DO: Run both the embedded and hidden through LSTM\n",
        "        output, hidden = None\n",
        "\n",
        "        # Return both output and hidden\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMBcVqtBkdm5"
      },
      "outputs": [],
      "source": [
        "# Encoder test\n",
        "dummy_in = torch.randint(1, 10, (6, 5), device=device)\n",
        "dummy_encoder = EncoderRNN(10, 300, 1024).to(device)\n",
        "dummy_out, dummy_hid = dummy_encoder.forward(dummy_in)\n",
        "assert dummy_out.shape == (6, 5, 2 * 1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ekxl5MczGpy"
      },
      "source": [
        "We use an LSTM decoder. The model definitions will be very similar except that we add a linear layer to project the hidden representations to the output tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxuXcv_4kgLk"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, output_size, emb_size, hidden_size, num_layers=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # TO-DO: Create nn.Embedding layer with (output_size, emb_size)\n",
        "        self.embedding = None\n",
        "\n",
        "        # TO-DO: Create nn.LSTM layer with (emb_size, hidden_size)\n",
        "        # Make sure to set batch_first=True\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=# TO-DO,\n",
        "            hidden_size=# TO-DO,\n",
        "            num_layers=# TO-DO,\n",
        "            batch_first=True,\n",
        "            bidirectional=False,\n",
        "            dropout=# TO-DO,\n",
        "        )\n",
        "\n",
        "        # TO-DO: Create a nn.Linear layer with (hidden_size, output_size)\n",
        "        self.proj = None\n",
        "\n",
        "    def forward(self, inputs, hidden):\n",
        "        # TO-DO: Run the input through the embedding layer\n",
        "        inputs = None\n",
        "\n",
        "        # TO-DO: Run both the input and hidden through LSTM\n",
        "        outputs, hidden = None\n",
        "\n",
        "        # TO-DO: Run the output through the linear layer\n",
        "        outputs = None\n",
        "\n",
        "        # Return both output and hidden\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4zGCwlmJcPI"
      },
      "source": [
        "### Checkpoint 2\n",
        "\n",
        "You should now be able to pass data through both the encoder and decoder.\n",
        "\n",
        "Note that the number of layers of the decoder should double that of the encoder, because the encoder is bidirectional while the decoder is not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_2N0WjiIxDo"
      },
      "outputs": [],
      "source": [
        "# Decoder test\n",
        "dummy_out = torch.randint(1, 10, (6, 7), device=device)\n",
        "dummy_decoder = DecoderRNN(10, 300, 1024).to(device)\n",
        "dummy_pred, _ = dummy_decoder.forward(dummy_out, dummy_hid)\n",
        "print(dummy_pred.shape)\n",
        "assert dummy_pred.shape == (6, 7, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaYxxOpPzJII"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "We now prepare the training loop. We need to first define a datastructure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUKin5LkMAay"
      },
      "outputs": [],
      "source": [
        "class MTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, src, tgt):\n",
        "        self.src = src\n",
        "        self.tgt = tgt\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {\"src\": self.src[index], \"tgt\": self.tgt[index]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmpGgSvXMAay"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    src = torch.nn.utils.rnn.pad_sequence(\n",
        "        [torch.tensor(b[\"src\"]) for b in batch], batch_first=True\n",
        "    )\n",
        "    tgt = torch.nn.utils.rnn.pad_sequence(\n",
        "        [torch.tensor(b[\"tgt\"]) for b in batch], batch_first=True\n",
        "    )\n",
        "    return src, tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxt8tAc5KZRw"
      },
      "source": [
        "Helper functions to convert the sentences into vector. Given a list of sentence, we use a Sentencepiece model to tokenize it and convert it into a list of word IDs.\n",
        "\n",
        "Check [Sentencepiece documentations](https://github.com/google/sentencepiece/blob/master/python/README.md#usage) to see how to use the models to convert text into ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWLLb5nPk2bh"
      },
      "outputs": [],
      "source": [
        "def preprocess(sp, data):\n",
        "    # TO-DO: use the Sentencepiece model to tokenize and turn the sentences into ids\n",
        "    # remember to add bos and eos tokens (from the Sentencepiece model) to the beginning and end of each sentence\n",
        "    raise NotImplemented\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ9VcIcgzStu"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Implement the main training loop here. This function takes in one batch of input and target tensors and does a forward pass, backward pass, and weight updates. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N-h0dp0k8wo"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    input_tensor,\n",
        "    target_tensor,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    use_teacher_forcing,\n",
        "    training=True,\n",
        "):\n",
        "    if training:\n",
        "        # TO-DO: Reset/Zero parameter gradients of the optimizer. Hint: https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html#zero-the-gradients-while-training-the-network\n",
        "        raise NotImplemented\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor)\n",
        "    dec_length = target_tensor.size()[-1]\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        # Remember that we are giving target[:-1] as the input, and matching targe[1:] with the output\n",
        "        \n",
        "        # TO-DO: Run decoder by providing target and encoder_hidden as input\n",
        "        decoder_output, _ = None\n",
        "\n",
        "        # TO-DO: Calculate loss\n",
        "        loss = None\n",
        "\n",
        "    else:\n",
        "        dec_length = 0\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        decoder_input = torch.tensor(\n",
        "            [[SOS_token]] * target_tensor.size()[0], device=device\n",
        "        )\n",
        "        last_hidden = encoder_hidden\n",
        "        for di in range(target_tensor.size()[-1] - 1):\n",
        "            dec_length += 1\n",
        "            # TO-DO: Run decoder by providing decoder_input and decoder_hidden as input\n",
        "            decoder_output, last_hidden = None\n",
        "\n",
        "            # Take the top output of current timestep of decoder. This will be input to next timestep\n",
        "            topv, topi = decoder_output.topk(1, dim=-1)\n",
        "            decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output.squeeze(-2), target_tensor[:,di+1])\n",
        "            if not training and torch.sum(decoder_input) == 0:\n",
        "                break\n",
        "\n",
        "    if training:\n",
        "        # TO-DO: Backprop by calling backward() function on loss\n",
        "        raise NotImplemented\n",
        "\n",
        "        # TO-DO: Update weights using step() on both encoder_optimizer and decoder_optimizer\n",
        "        raise NotImplemented\n",
        "\n",
        "    return loss.item() / dec_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xV9QrE_lGyb"
      },
      "outputs": [],
      "source": [
        "def trainIters(\n",
        "    encoder,\n",
        "    decoder,\n",
        "    n_iters,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    learning_rate=0.001,\n",
        "    teacher_forcing_ratio=0.5,\n",
        "):\n",
        "    # Initialize AdamW optimizer for both encoder and decoder\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [{\"params\": encoder.parameters()}, {\"params\": decoder.parameters()}],\n",
        "        lr=learning_rate,\n",
        "    )\n",
        "\n",
        "    # We will be using cross entropy as the criterion\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    epoch_train_losses = []\n",
        "    epoch_valid_losses = []\n",
        "\n",
        "    # In each epoch, we go through all training examples\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        # Train\n",
        "        train_loss = []\n",
        "        for input_tensor, output_tensor in tqdm(train_loader):\n",
        "            use_teacherforcing = (\n",
        "                True if random.random() < teacher_forcing_ratio else False\n",
        "            )\n",
        "\n",
        "            loss = train(\n",
        "                input_tensor.to(device),\n",
        "                output_tensor.to(device),\n",
        "                encoder,\n",
        "                decoder,\n",
        "                optimizer,\n",
        "                criterion,\n",
        "                use_teacherforcing,\n",
        "            )\n",
        "            train_loss.append(loss)\n",
        "\n",
        "        # Validate\n",
        "        valid_loss = []\n",
        "        for input_tensor, output_tensor in tqdm(valid_loader):\n",
        "            loss = train(\n",
        "                input_tensor.to(device),\n",
        "                output_tensor.to(device),\n",
        "                encoder,\n",
        "                decoder,\n",
        "                optimizer,\n",
        "                criterion,\n",
        "                False,\n",
        "                training=False,\n",
        "            )\n",
        "            valid_loss.append(loss)\n",
        "\n",
        "        avg_train_loss = np.mean(train_loss)\n",
        "        avg_valid_loss = np.mean(valid_loss)\n",
        "\n",
        "        print(\n",
        "            \"Epoch: {}/{}. Avg Train Loss: {}. Avg Valid Loss: {}\".format(\n",
        "                iter, n_iters, avg_train_loss, avg_valid_loss\n",
        "            )\n",
        "        )\n",
        "\n",
        "        epoch_train_losses.append(avg_train_loss)\n",
        "        epoch_valid_losses.append(avg_valid_loss)\n",
        "\n",
        "    return epoch_train_losses, epoch_valid_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLCVURlPzlUb"
      },
      "source": [
        "Define the hyperparameters. You can try different settings.\n",
        "\n",
        "If you are running into memory issues, decrease the BATCH_SIZE or EMB_SIZE or HIDDEN_SIZE.\n",
        "\n",
        "If you have access to more powerful hardware, you can increase the BATCH_SIZE, EMB_SIZE, HIDDEN_SIZE, NUM_EPOCHS to train longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYye7pQOhNC2"
      },
      "outputs": [],
      "source": [
        "# Model Training Hyper-Parameters\n",
        "MAX_LENGTH = 20\n",
        "EMB_SIZE = 100\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 2\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "DROPOUT = 0.3\n",
        "TEACHER_FORCING_RATIO = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRXO20epC-V-"
      },
      "source": [
        "Turn sequences of text into sequences of ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C4pGP0XMAay"
      },
      "outputs": [],
      "source": [
        "# reload thesentencepiece models if necessary\n",
        "en_sp = spm.SentencePieceProcessor(model_file=\"en-zh.en.model\")\n",
        "zh_sp = spm.SentencePieceProcessor(model_file=\"en-zh.zh.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJPqb8eLQBgO"
      },
      "outputs": [],
      "source": [
        "src = preprocess(en_sp, train_data[\"en\"])\n",
        "tgt = preprocess(zh_sp, train_data[\"zh\"])\n",
        "train_dataset = MTDataset(src, tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7IiXc67pnDF"
      },
      "outputs": [],
      "source": [
        "src = preprocess(en_sp, valid_data[\"en\"])\n",
        "tgt = preprocess(zh_sp, valid_data[\"zh\"])\n",
        "valid_dataset = MTDataset(src, tgt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xij73iGGSBX"
      },
      "source": [
        "Generate training and validation splits and dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwPvXKWeGSYo"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyUlEf2QDLcx"
      },
      "source": [
        "Now we can create the models and start the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFX50FjYMAaz"
      },
      "outputs": [],
      "source": [
        "encoder_eng_zh = EncoderRNN(en_sp.vocab_size(), EMB_SIZE, HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
        "decoder_eng_zh = DecoderRNN(zh_sp.vocab_size(), EMB_SIZE, HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
        "SOS_token = en_sp.bos_id()\n",
        "\n",
        "avg_train_losses_zh, avg_valid_losses_zh = trainIters(\n",
        "    encoder_eng_zh, decoder_eng_zh, NUM_EPOCHS, train_loader, valid_loader, learning_rate=LEARNING_RATE, teacher_forcing_ratio=TEACHER_FORCING_RATIO\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoxlVmdLDdaE"
      },
      "source": [
        "Save your models and training log after training. You should download the weights and logs from Colab so you don't have to train the model again when you start working on the rest of the notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vscJES10D45a"
      },
      "outputs": [],
      "source": [
        "torch.save(encoder_eng_zh, 'en_zh_encoder.pt')\n",
        "torch.save(decoder_eng_zh, 'en_zh_decoder.pt')\n",
        "\n",
        "with open(\"losses_en_zh.txt\", \"w\") as fp:\n",
        "    for i, j in zip(avg_train_losses_zh, avg_valid_losses_zh):\n",
        "        fp.write(\"{:.12f} {:.12f}\\n\".format(i, j))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW17lwRuEcNh"
      },
      "source": [
        "### Checkpoint 3\n",
        "\n",
        "Use this function to plot the training and evaluation losses. What trends can you observe?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, valid_losses):\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, ax = plt.subplots()\n",
        "    x = np.arange(len(train_losses)) + 1\n",
        "    ax.plot(x, train_losses, 'o-', label='training')\n",
        "    ax.plot(x, valid_losses, 'o-', label='validation')\n",
        "    ax.set_xlabel('epochs')\n",
        "    ax.set_ylabel('loss values')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WzrtphY_tvbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(avg_train_losses_zh, avg_valid_losses_zh)"
      ],
      "metadata": {
        "id": "mLlgaoNztwvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ehweWG2ztnl"
      },
      "source": [
        "## Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the baseline decoding method that generate the tokens greedily."
      ],
      "metadata": {
        "id": "cgXkV6X6j8xG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX1gbuhRgC2a"
      },
      "outputs": [],
      "source": [
        "def predict(encoder, decoder, sentence, src_sp, tgt_sp):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = [[src_sp.bos_id()] + src_sp.encode_as_ids(sentence) + [src_sp.eos_id()]]\n",
        "        input_tensor = torch.tensor(input_tensor, device=device)\n",
        "\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_input = torch.tensor([[tgt_sp.bos_id()]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_ids = []\n",
        "\n",
        "        for di in range(MAX_LENGTH):\n",
        "            # TO-DO: generate next output and hidden state from decoder_input and decoder_hidden\n",
        "            decoder_output, decoder_hidden = None\n",
        "\n",
        "            # TO-DO: get the id of the most likely item from decoder_output\n",
        "            _, topi = None\n",
        "            if topi.item() == tgt_sp.eos_id():\n",
        "                break\n",
        "            else:\n",
        "                decoded_ids.append(topi.item())\n",
        "\n",
        "            decoder_input = topi.squeeze(-1).detach()\n",
        "        # TO-DO: use the Sentencepiece model to convert the ids back to a string\n",
        "        decoded_words = None\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus points (20%): implement beam search. This is optional.\n",
        "\n",
        "To make autograding work, your beam_search should return a list of predictions from the most likely to the least likely, and each prediction should be a tuple of (score, text). For example:\n",
        "\n",
        "```\n",
        "[(0,2, \"Thank you\"), (0.1, \"Thanks\"), (0.01, \"Thanks you\")]\n",
        "```"
      ],
      "metadata": {
        "id": "0LEO_r6AkaSB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSs9bH5pdRho"
      },
      "outputs": [],
      "source": [
        "def beam_search(encoder, decoder, sentence, src_sp, tgt_sp, beam_size=5):\n",
        "    assert beam_size > 1, \"if beam_size = 1, then that's greedy search\"\n",
        "    with torch.no_grad():\n",
        "        # TO-DO: Just like predict() but instead of the topk with 1, take topk with beam_size and rank sort the beam for the more likely probability\n",
        "        raise NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN6VGxL4z3JY"
      },
      "source": [
        "Try out a random input. \n",
        "\n",
        "1. Does the output make sense if given an input from the training set?\n",
        "2. Does the output make sense if given an arbitrary input?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU4WSPHXHb5f"
      },
      "outputs": [],
      "source": [
        "# reload the encoder and decoder if necessary\n",
        "encoder_eng_zh = torch.load('en_zh_encoder.pt')\n",
        "decoder_eng_zh = torch.load('en_zh_decoder.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tge_5xmLrSkn"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    predict(\n",
        "        encoder_eng_zh,\n",
        "        decoder_eng_zh,\n",
        "        \"What gives us the courage?\",\n",
        "        en_sp,\n",
        "        zh_sp,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3H9Ac7eMHeL"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    beam_search(\n",
        "        encoder_eng_zh,\n",
        "        decoder_eng_zh,\n",
        "        \"What gives us the courage?\",\n",
        "        en_sp,\n",
        "        zh_sp,\n",
        "        beam_size=5,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rITuKy0o_ayD"
      },
      "source": [
        "Define a function to generate model predictions and save them in a file. Download your model predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7vE5rlDhsFx"
      },
      "outputs": [],
      "source": [
        "def generate_and_save(valid_inputs, encoder, decoder, src_sp, tgt_sp, beam_size=1, saveloc=\"preds.zh\"):\n",
        "    generated_sents = []\n",
        "    for src_sent in tqdm(valid_inputs):\n",
        "        if beam_size <= 1:\n",
        "            tgt_sent = predict(encoder, decoder, src_sent, src_sp, tgt_sp)\n",
        "        else:\n",
        "            # change this if your beam search returns something different\n",
        "            tgt_sent = beam_search(encoder, decoder, src_sent, src_sp, tgt_sp, beam_size)[0][1]\n",
        "        # if the decoded string is empty, we replace it with the Sentencepiece unk character\n",
        "        if len(tgt_sent) == 0:\n",
        "            tgt_sent = \"⁇\"\n",
        "        generated_sents.append(tgt_sent)\n",
        "    with open(saveloc, \"w\") as fout:\n",
        "        fout.write(\"\\n\".join(generated_sents))\n",
        "    return generated_sents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxzpFaKQ0EjN"
      },
      "source": [
        "### Checkpoint 4\n",
        "\n",
        "Calculate BLEU and CHRF scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpWdzp-nuoS5"
      },
      "outputs": [],
      "source": [
        "with open(\"valid.zh\") as fin:\n",
        "    refs = fin.read().strip().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EpUbol2u1nh"
      },
      "outputs": [],
      "source": [
        "preds = generate_and_save(valid_data[\"en\"], encoder_eng_zh, decoder_eng_zh, en_sp, zh_sp, saveloc=\"valid_preds.zh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BLEU score will be very low for the baseline model."
      ],
      "metadata": {
        "id": "dN6XTgEazwth"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcfJYlvytFrc"
      },
      "outputs": [],
      "source": [
        "bleu = BLEU(tokenize=\"flores101\")\n",
        "print(bleu.corpus_score(preds, refs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should get at least 25 CHRF score."
      ],
      "metadata": {
        "id": "PJQUow9Dz2iH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRRB5hjwvQcN"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "print(chrf.corpus_score(preds, refs))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}