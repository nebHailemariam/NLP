{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "57c02bbc",
      "metadata": {
        "id": "57c02bbc"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yVlDbPESEmKU",
      "metadata": {
        "id": "yVlDbPESEmKU"
      },
      "source": [
        "* This assignment focuses on building a simple **Extractive Question Answering** model using **Huggingface** and **Pytorch**. \n",
        "Given a context paragraph and a question based on it, the task is to extract the answer from the context.\n",
        "\n",
        "* The main aim of the assignment is to be familiar with the basic coding concepts in Huggingface and design an inference pipeline for QA. \n",
        "\n",
        "* You are required to do the following things: \n",
        "    * Download a pretrained model from [Huggingface Model Hub](https://huggingface.co/models).\n",
        "    * Design the pre-processing pipeline.\n",
        "    * Design the post-processing pipeline.\n",
        "    * Perform inference on [SQuAD 2.0](https://arxiv.org/abs/1806.03822) dataset.\n",
        "    * Get the results on the blind test set.\n",
        "\n",
        "\n",
        "* Students are required to complete the coding sections which have been marked with `#TODO`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d068f1",
      "metadata": {
        "id": "c5d068f1"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hABOdgo4FXKL",
      "metadata": {
        "id": "hABOdgo4FXKL"
      },
      "source": [
        "First we need to install ðŸ¤— Transformers, ðŸ¤— Datasets, and ðŸ¤— evaluate libraries from Huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "521d6cc6",
      "metadata": {
        "id": "521d6cc6",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (2.9.0)\n",
            "Requirement already satisfied: transformers in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (4.26.0)\n",
            "Requirement already satisfied: evaluate in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (0.4.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (2.28.2)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: xxhash in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: dill<0.3.7 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (1.24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "  Using cached torchvision-0.14.1-cp38-cp38-win_amd64.whl (1.1 MB)\n",
            "Requirement already satisfied: numpy in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from torchvision) (1.24.1)\n",
            "Collecting pillow!=8.3.*,>=5.3.0\n",
            "  Downloading Pillow-9.4.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
            "Requirement already satisfied: requests in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from torchvision) (2.28.2)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from torchvision) (4.4.0)\n",
            "Collecting torch==1.13.1\n",
            "  Using cached torch-1.13.1-cp38-cp38-win_amd64.whl (162.6 MB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\lib\\site-packages (from requests->torchvision) (3.4)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "Successfully installed pillow-9.4.0 torch-1.13.1 torchvision-0.14.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\nebiyou hailemariam\\desktop\\development\\nlp\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets transformers evaluate\n",
        "! pip install torchvision "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca5e9ff5",
      "metadata": {
        "id": "ca5e9ff5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gFy7bOpvEgbT",
      "metadata": {
        "id": "gFy7bOpvEgbT"
      },
      "source": [
        "We start by importing necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4d1c897d",
      "metadata": {
        "id": "4d1c897d"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import collections\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import (\n",
        "    DataLoader,\n",
        "    Dataset\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3SI6gslLEk_8",
      "metadata": {
        "id": "3SI6gslLEk_8"
      },
      "source": [
        "## Setting up the GPU\n",
        "\n",
        "Following that, we find the available GPUs and save the information in the `DEVICE` variable. This will be useful later on when we need to move tensors and models from CPU to GPU and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0172d02a",
      "metadata": {
        "id": "0172d02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPU:  cpu\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(\"Using GPU: \", DEVICE)\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(\"Using CPU: \", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yDqtO07xpk4s",
      "metadata": {
        "id": "yDqtO07xpk4s"
      },
      "source": [
        "## Seeding the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "PfzV-Uwnposp",
      "metadata": {
        "id": "PfzV-Uwnposp"
      },
      "outputs": [],
      "source": [
        "def set_random_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Helper function to seed experiment for reproducibility.\n",
        "    If -1 is provided as seed, experiment uses random seed from 0~9999\n",
        "    Args:\n",
        "        seed (int): integer to be used as seed, use -1 to randomly seed experiment\n",
        "    \"\"\"\n",
        "    print(\"Seed: {}\".format(seed))\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "eSkx-qrDp1aD",
      "metadata": {
        "id": "eSkx-qrDp1aD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed: 0\n"
          ]
        }
      ],
      "source": [
        "SEED = 0\n",
        "set_random_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f644d0",
      "metadata": {
        "id": "93f644d0"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2hafBOVDFq6Q",
      "metadata": {
        "id": "2hafBOVDFq6Q"
      },
      "source": [
        "We start by declaring a config dictionary containing all the hyperparameters required for data preprocessing and model inference. They are as follows:\n",
        "\n",
        "1. `model_checkpoint`: The model to be used for from [Huggingface Model Hub](https://huggingface.co/models) for our QA task. We recommend using models the [RoBERTa Model](https://huggingface.co/deepset/roberta-base-squad2) that is already fine-tuned on the SQuAD datasets for good performance.\n",
        "2. `max_length`: The maximum length of the input sequence. If left unset, the tokenizer will use the predefined model maximum length. (Ideally set between 300-512)\n",
        "3. `truncation`: Determines whether to truncate the input sequence or not. See the documentation for details on the different values it can accept.\n",
        "4. `padding`: Determines whether to pad the input sequence or not. See the documentation for details on the different values it can accept.\n",
        "5. `return_overflowing_tokens`: Determines whether to return overflowing token sequences after truncation/ when the input sequence exceeds the maximum length.\n",
        "6. `return_offsets_mapping`: Determines whether to return (char_start, char_end) for each token in the input sequence.\n",
        "7. `stride`: The number of overlapping tokens between the truncated and the overflowing sequences.\n",
        "8. `n_best_size`: The top 'n' answers to select from the predictions.\n",
        "9. `max_answer_length`: The maximum length of the answer.\n",
        "10. `batch_size`: The number of examples to be included in each batch. It should be selected properly such that the batch fits into the GPU. (Ideally from 16 to 128)\n",
        "\n",
        "Check out the following links for more information on Tokenizers in huggingface.\n",
        "1. [Summary of Tokenizers](https://huggingface.co/docs/transformers/v4.24.0/en/tokenizer_summary)\n",
        "2. [Padding and Truncation](https://huggingface.co/docs/transformers/v4.24.0/en/pad_truncation)\n",
        "3. [Batch Encoding](https://huggingface.co/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_encode_plus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f323c24",
      "metadata": {
        "id": "8f323c24"
      },
      "outputs": [],
      "source": [
        "# TODO 1: fill in the values for all the hyper-paramters mentioned in the config dictionary.\n",
        "config = {\n",
        "    'model_checkpoint': ,\n",
        "    \"max_length\": ,\n",
        "    \"truncation\": ,\n",
        "    \"padding\": ,\n",
        "    \"return_overflowing_tokens\": ,\n",
        "    \"return_offsets_mapping\": ,\n",
        "    \"stride\": ,\n",
        "    \"n_best_size\": ,\n",
        "    \"max_answer_length\": ,\n",
        "    \"batch_size\": \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zfg5b79OF5QV",
      "metadata": {
        "id": "zfg5b79OF5QV"
      },
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "For this assignment, we will be using [SQuAD](https://arxiv.org/abs/1606.05250), an academic benchmark for extractive question answering. We will use the [SQuAD 2.0](https://arxiv.org/abs/1806.03822), an updated version of the dataset containing harder examples as well as examples which do not have answers in the context.\n",
        "\n",
        "We will use the `load_dataset` function from [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to load the dataset.\n",
        "\n",
        "The `load_dataset` function returns a [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict) object, which conatins the *train* and *validation* splits for the dataset. We will be using only the *validation* split in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed50f1cb",
      "metadata": {
        "id": "ed50f1cb"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "datasets = load_dataset(\"squad_v2\")\n",
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iI1LM5LryvQC",
      "metadata": {
        "id": "iI1LM5LryvQC"
      },
      "source": [
        "## Dataset Inspection\n",
        "\n",
        "You can select any example in the dataset by specifying the split and the example index.\n",
        "The code below prints a randomly selected example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30dd12b6",
      "metadata": {
        "id": "30dd12b6"
      },
      "outputs": [],
      "source": [
        "index = random.randint(0, len(datasets['validation']))\n",
        "datapoint = datasets['validation'][index]\n",
        "print(f\"index: {index}\\n\")\n",
        "for column, info in datapoint.items():\n",
        "    print(f\"\\n{column}:\\t{info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qB2n3RdPzQQG",
      "metadata": {
        "id": "qB2n3RdPzQQG"
      },
      "source": [
        "We can see that each example contains 4 fields, namely:\n",
        "1.  ***id*** (a unique identifier for each example)\n",
        "2. ***title*** (the genre of the example)\n",
        "3. ***context*** (The paragraph on which the question is asked)\n",
        "4. ***question*** (the actual question the ML model needs to answer)\n",
        "5. ***answer*** (the actual answer to the question indicated in text as well as its start and end position in the context). During training, there is only one possible answer. For evaluation, however, there are several possible answers for each sample, which may be the same or different.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z0HDMnxl01q5",
      "metadata": {
        "id": "Z0HDMnxl01q5"
      },
      "source": [
        "## Loading the Tokenizer and the Model\n",
        "\n",
        "Next, we download the RoBERTa model fine-tuned on SQuAD along with its tokenizer. Refer to the [Huggingface documentation](https://huggingface.co/docs/transformers/autoclass_tutorial) to see how to load pretrained models and tokenizers.\n",
        "\n",
        "The ðŸ¤— Transformers `Tokenizer` tokenizes the input sequence and converts the tokens to their corresponding IDs in the pretrained vocabulary. It generates various inputs that a model requires such as input_ids, attention_mask, token_type_ids, etc. You can read more details about this in the Huggingface [Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer) and [Preprocessing](https://huggingface.co/docs/transformers/preprocessing) documentation.\n",
        "\n",
        "The ðŸ¤— Transformers `AutoModelForQuestionAnswering` is a transformer model with a span classification head for extractive question answering. It returns ***start_logits*** and ***end_logits***, marking the start and end of the answer, respectively. More details on this model is given [here](https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaForQuestionAnswering)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55b7f42",
      "metadata": {
        "id": "d55b7f42"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# TODO 2: Define the tokenizer and QA model. Transfer the QA model to GPU.\n",
        "# tokenizer = \n",
        "# qa_model = "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QN89Xnd71fzj",
      "metadata": {
        "id": "QN89Xnd71fzj"
      },
      "source": [
        "## Dataset Class\n",
        "\n",
        "Next we define the a custom Dataset class for our SQuAD corpus.\n",
        "\n",
        "The class implements three main functions: \n",
        "1. `__init__`: This function is run once when instantiating the Dataset object. We generally initialize our raw dataset, tokenizer, and tokenized dataset in this function. \n",
        "2. `__len__`: This returns the total number of examples in our dataset. Note that we set it to the number of available ***input_ids*** and not the size of the raw dataset. This is because we are allowing context longer than maximum length of the model, resulting in increased number of features.\n",
        "3. `__getitem__`: This function return the sample from our dataset at a given index. You are supposed to implement this function.\n",
        "\n",
        "You don't need to understand the details of each of them for the purpose of this assignment. However, if you want to learn more, you can lookup the [PyTorch Documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qyCNEwkn6ku_",
      "metadata": {
        "id": "qyCNEwkn6ku_"
      },
      "source": [
        "### Important steps for preprocessing QA data: (`TODO`)\n",
        "\n",
        "1. As QA task contains two input fields of the question and the context, we concatenate both of them to pass it to the model. Thus, the input is `[CLS] Question [SEP] Context [SEP]`. Note that we never want to truncate the question, only the context. So choose the `truncation` hyper-parameter in the [config cell](https://colab.research.google.com/drive/1HQ9z8cZE8TgLjlkAekEXfih9x9byB65r#scrollTo=8f323c24&line=2&uniqifier=1) accordingly.\n",
        "2. As a result, in the case of very long documents, we must be careful not to lose the context that contains the answer. To resolve this concern, we will allow longer examples in our dataset to provide multiple input features, each of which is shorter than the maximum length (set as a hyper-parameter in the config dictionary). This can be done using the `stride` and `return_overflowing_tokens` hyper-parameters.\n",
        "3. The `sequence ids` in the tokenized input can be used to distinguish between the various sequences in an input example. In our case, the question will be assigned 0 and the context will be assigned 1, because the former comes after the latter in the sequence. We know that the answer tokens always lie in the context. Hence, to make things easy for post-processing, we set the offset mapping of the tokens that are not a part of the context to -1.\n",
        "4. The `overflow_to_sample_mapping` key return by the tokenizer is useful to map each feature we get to its corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "916ff6d7",
      "metadata": {
        "id": "916ff6d7"
      },
      "outputs": [],
      "source": [
        "class QADataset(Dataset):\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        data,\n",
        "        tokenizer,\n",
        "        config\n",
        "    ):\n",
        "\n",
        "        self.config = config\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenized_data = self.tokenizer(\n",
        "            self.data[\"question\"],\n",
        "            self.data[\"context\"],\n",
        "            max_length=self.config[\"max_length\"],\n",
        "            stride=self.config[\"stride\"],\n",
        "            truncation=self.config[\"truncation\"],\n",
        "            padding=self.config[\"padding\"],\n",
        "            return_overflowing_tokens=self.config[\"return_overflowing_tokens\"],\n",
        "            return_offsets_mapping=self.config[\"return_offsets_mapping\"],\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "        \n",
        "        example_ids = []\n",
        "        for i, sample_mapping in enumerate(tqdm(self.tokenized_data[\"overflow_to_sample_mapping\"])):\n",
        "            example_ids.append(self.data[\"id\"][sample_mapping])\n",
        "\n",
        "            sequence_ids = self.tokenized_data.sequence_ids(i)\n",
        "            offset_mapping = self.tokenized_data[\"offset_mapping\"][i]\n",
        "            \n",
        "            # TODO 3: set the offset mapping of the tokenized data at index i to (-1, -1) \n",
        "            # if the token is not in the context\n",
        "            \n",
        "\n",
        "        self.tokenized_data[\"ID\"] = example_ids\n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(\n",
        "        self\n",
        "    ):\n",
        "        # TODO 4: define the length of the dataset equal to total number of unique features (not the total number of datapoints)\n",
        "    \n",
        "    \n",
        "    \n",
        "    def __getitem__(\n",
        "        self,\n",
        "        index: int\n",
        "    ):\n",
        "        # TODO 5: Return the tokenized dataset at the given index. Convert the various inputs to tensor using torch.tensor\n",
        "        return {\n",
        "            'input_ids': ,\n",
        "            'attention_mask': ,\n",
        "            'offset_mapping': ,\n",
        "            'example_id': ,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ElvCQm5k-ZvH",
      "metadata": {
        "id": "ElvCQm5k-ZvH"
      },
      "source": [
        "## Creating Dataloader\n",
        "\n",
        "1. We create an object of our custom QADataset class.\n",
        "2. To access examples batch-wise, we create a [Dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) object that is an iterable around the Dataset object. The `Dataloader` takes the `Dataset object` and `batch size` as parameters. There are many other parameters that one can specify but we only need batch size for this assignment.\n",
        "3. Note that the length of the Dataloader object multiplied by the batch size should approximately give you the size of the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b77635e",
      "metadata": {
        "id": "5b77635e"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "eval_dataset = QADataset(\n",
        "    data=datasets['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503243aa",
      "metadata": {
        "id": "503243aa"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset,\n",
        "    batch_size=config[\"batch_size\"]\n",
        ")\n",
        "len(eval_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xlf36Qfz_eOD",
      "metadata": {
        "id": "Xlf36Qfz_eOD"
      },
      "source": [
        "We collect the raw and tokenized dataset in seperate variables as they will be required during post-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab303c22",
      "metadata": {
        "id": "ab303c22"
      },
      "outputs": [],
      "source": [
        "eval_data = eval_dataset.data\n",
        "eval_features = eval_dataset.tokenized_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9762de5a",
      "metadata": {
        "id": "9762de5a"
      },
      "source": [
        "# Inference on SQuAD (`TODO`)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zhKN8s8UABmB",
      "metadata": {
        "id": "zhKN8s8UABmB"
      },
      "source": [
        "* In the cell below you are supposed to perform inference on the SQuAD valdiation set.\n",
        "* You are supposed to iterate over the DataLoader object, pass the tokenized input to the model, and store the start and end logits.\n",
        "* Note: Do not forget to transfer the start and end logits tensor from GPU to CPU. Convert them to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67c11768",
      "metadata": {
        "id": "67c11768"
      },
      "outputs": [],
      "source": [
        "def qa_inference(model, data_loader):\n",
        "    model.eval()\n",
        "    start_logits = []\n",
        "    end_logits = []\n",
        "    for step, batch in enumerate(tqdm(data_loader, desc=\"Inference Iteration\")):\n",
        "        with torch.no_grad():\n",
        "            model_kwargs = {\n",
        "                'input_ids': batch['input_ids'].to(DEVICE, dtype=torch.long),\n",
        "                'attention_mask': batch['attention_mask'].to(DEVICE, dtype=torch.long)\n",
        "            }    \n",
        "\n",
        "            # TODO 6: pass the model arguments to the model and store the output\n",
        "\n",
        "            # TODO 7: Extract the start and end logits by extending `start_logits` and `end_logits`\n",
        "\n",
        "    # TODO 8: Convert the start and end logits to a numpy array (by passing them to `np.array`)\n",
        "\n",
        "    # TODO 9: return start and end logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H76zhVTqJfmc",
      "metadata": {
        "id": "H76zhVTqJfmc"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "start_logits, end_logits = qa_inference(qa_model, eval_dataloader)\n",
        "start_logits.shape, end_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zLFPDuzzBefk",
      "metadata": {
        "id": "zLFPDuzzBefk"
      },
      "source": [
        "# Postprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XqjyNJY7ENJx",
      "metadata": {
        "id": "XqjyNJY7ENJx"
      },
      "source": [
        "The predictions could give rise to various difficulties:\n",
        "1. The answer span could be the text in the question.\n",
        "2. Answer would be too long.\n",
        "3. The start position could be greater than the end position.\n",
        "\n",
        "We have to do the following postprocessing steps to avoid the abouve mentioned senarios:\n",
        "1. Skip answers that are not fully in the context (Hint: make use of the modified offset mapping done in the [preprocessing step](https://colab.research.google.com/drive/1HQ9z8cZE8TgLjlkAekEXfih9x9byB65r#scrollTo=916ff6d7&line=1&uniqifier=1)).\n",
        "2. To select the best possible start and end logits, first sort them and select the top 'n' choices using the `n_best_size` hyper-paramter. Then iterate over the start and end logits and skip the answers with a length that is either < 0 or > `max_answer_length`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb6a326",
      "metadata": {
        "id": "8fb6a326"
      },
      "outputs": [],
      "source": [
        "def post_processing(raw_dataset, tokenized_dataset, start_logits, end_logits):\n",
        "    \n",
        "    # Map each example to its features. This is done because an example can have multiple features\n",
        "    # as we split the context into chunks if it exceeded the max length\n",
        "    data2features = collections.defaultdict(list)\n",
        "    for idx, feature_id in enumerate(tokenized_dataset['ID']):\n",
        "        data2features[feature_id].append(idx)\n",
        "\n",
        "    # Decode the answers for each datapoint\n",
        "    predictions = []\n",
        "    for data in tqdm(raw_dataset):\n",
        "        answers = []\n",
        "        data_id = data[\"id\"]\n",
        "        context = data[\"context\"]\n",
        "\n",
        "        for feature_index in data2features[data_id]:\n",
        "\n",
        "            # TODO 10: Get the start logit, end logit, and offset mapping for each index.\n",
        "\n",
        "\n",
        "            # TODO 11: Sort the start and end logits and get the top n_best_size logits.\n",
        "            # Hint: look at other QA pipelines/tutorials.\n",
        "            \n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    \n",
        "                    # TODO 12: Exclde answers that are not in the context\n",
        "                    \n",
        "                    # TODO 13: Exclude answers if (answer length < 0) or (answer length > max_answer_length)\n",
        "                    \n",
        "\n",
        "                    # TODO 14: collect answers in a list.\n",
        "                    answers.append(\n",
        "                        {\n",
        "                            \"text\": ,\n",
        "                            \"logit_score\": ,\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "        best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "        predictions.append(\n",
        "            {\n",
        "                \"id\": data_id, \n",
        "                \"prediction_text\": best_answer[\"text\"],\n",
        "                \"no_answer_probability\": 0.0 if len(best_answer[\"text\"]) > 0 else 1.0\n",
        "            }\n",
        "        )    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e77a9cac",
      "metadata": {
        "id": "e77a9cac"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "predicted_answers = post_processing(\n",
        "    raw_dataset=eval_data, \n",
        "    tokenized_dataset=eval_features,\n",
        "    start_logits=start_logits,\n",
        "    end_logits=end_logits\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f245c7",
      "metadata": {
        "id": "a1f245c7"
      },
      "outputs": [],
      "source": [
        "gold_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in eval_data][:len(eval_data)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22706b93",
      "metadata": {
        "id": "22706b93"
      },
      "outputs": [],
      "source": [
        "assert len(predicted_answers) == len(gold_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a4c1371",
      "metadata": {
        "id": "1a4c1371"
      },
      "outputs": [],
      "source": [
        "print(predicted_answers[0])\n",
        "print(gold_answers[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DMCKAFbpMR1V",
      "metadata": {
        "id": "DMCKAFbpMR1V"
      },
      "source": [
        "## Evaluating the Predictions\n",
        "\n",
        "We use the `ðŸ¤— Evaluate` library from Huggingace for evaluating our predictions. \n",
        "Specifically, we evaluate the model based on two metrics:\n",
        "1. `exact match`: This metric measures the percentage of predictions that match any one of the ground truth answers exactly.\n",
        "2. `macro-averaged f1 score`: This metric mea- sures the average overlap between the prediction and ground truth answer. We treat the prediction and ground truth as bags of tokens, and compute their F1. We take the maximum F1 over all of the ground truth answers for a given question, and then average over all of the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "346393b2",
      "metadata": {
        "id": "346393b2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "eval_metric = load(\"squad_v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "399d5c6e",
      "metadata": {
        "id": "399d5c6e"
      },
      "outputs": [],
      "source": [
        "eval_results = eval_metric.compute(predictions=predicted_answers, references=gold_answers)\n",
        "eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ewGht9nbopMQ",
      "metadata": {
        "id": "ewGht9nbopMQ"
      },
      "source": [
        "Save the SQuAD results as a json file. Make use to name the file as `squad_results.json`. \n",
        "\n",
        "Make sure to download the json file and upload on gradscope with the same name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_VaIG_ZPo8Hm",
      "metadata": {
        "id": "_VaIG_ZPo8Hm"
      },
      "outputs": [],
      "source": [
        "#TODO 15: save the metric results in squad_results.json file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nhpDEZBeNPy-",
      "metadata": {
        "id": "nhpDEZBeNPy-"
      },
      "source": [
        "# Blind Test Set\n",
        "\n",
        "Now you will be given a blind test set for which you need to generate appropriate predictions using the functions given above.\n",
        "\n",
        "You should be able to use code snippets from the SQuAD evaluation section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S0gm77axNVMA",
      "metadata": {
        "id": "S0gm77axNVMA"
      },
      "source": [
        "## Load and preprocess the dataset\n",
        "\n",
        "You can load the blind test set just like the SQuAD corpus using the `load_dataset` function from the `ðŸ¤— Datasets` library. \n",
        "\n",
        "More information on how to load a csv file using the load_dataset function is given [here](https://huggingface.co/docs/datasets/loading#csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a1afff5",
      "metadata": {
        "id": "4a1afff5"
      },
      "outputs": [],
      "source": [
        "# TODO 16: Load the blind test dataset using the load_dataset function, make sure to mention the split as `train`.\n",
        "# test_dataset ="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o6ebX5rfZgDG",
      "metadata": {
        "id": "o6ebX5rfZgDG"
      },
      "source": [
        "The code below prints a randomly selected example in the test set.\n",
        "\n",
        "As you can see, the test data only contains id, title, context, and question and not the answers. Your job is to generate appropriate answers for the blind test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0dd0043",
      "metadata": {
        "id": "f0dd0043"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "index = random.randint(0, len(test_dataset))\n",
        "datapoint = test_dataset[index]\n",
        "print(f\"index: {index}\\n\")\n",
        "for column, info in datapoint.items():\n",
        "    print(f\"\\n{column}:\\t{info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V_Ak10zgkrKM",
      "metadata": {
        "id": "V_Ak10zgkrKM"
      },
      "source": [
        "* Wrap the test data into the custom QADataset object.\n",
        "* Create a dataloader to loop over the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8487fba",
      "metadata": {
        "id": "d8487fba"
      },
      "outputs": [],
      "source": [
        "# TODO 17: Define the QADataset object for the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6db36e4",
      "metadata": {
        "id": "d6db36e4"
      },
      "outputs": [],
      "source": [
        "# TODO 18: Define the dataloader for the test set "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "byS-1qaenfPX",
      "metadata": {
        "id": "byS-1qaenfPX"
      },
      "source": [
        "Collect the raw and tokenized dataset in seperate variables as they will be required during post-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595ed7de",
      "metadata": {
        "id": "595ed7de"
      },
      "outputs": [],
      "source": [
        "# TODO 19: Save the raw and tokenized test set into seperate variables."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i8bgFLA8NTiy",
      "metadata": {
        "id": "i8bgFLA8NTiy"
      },
      "source": [
        "## Inference on Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kIkh0niZoNwq",
      "metadata": {
        "id": "kIkh0niZoNwq"
      },
      "source": [
        "Use the `qa_inference` function to generate the start and end logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0abc0793",
      "metadata": {
        "id": "0abc0793"
      },
      "outputs": [],
      "source": [
        "# TODO 20: perfom inference on the blind test to get the start and end logits (use the qa_inference function)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_OenxzJ_oOnl",
      "metadata": {
        "id": "_OenxzJ_oOnl"
      },
      "source": [
        "Use the `post_processing` function to generate the final candidate answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd634435",
      "metadata": {
        "id": "fd634435"
      },
      "outputs": [],
      "source": [
        "# TODO 21: post process the predictions to generate the candidate answers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GzQEsIPAoYmQ",
      "metadata": {
        "id": "GzQEsIPAoYmQ"
      },
      "source": [
        "Save the results as a json file. Make sure to name the file as `blind_test_predictions.json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd10090",
      "metadata": {
        "id": "2cd10090"
      },
      "outputs": [],
      "source": [
        "#TODO 22: Save the candidate answers in `blind_test_predictions.json` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fPBjc-gJRLPq",
      "metadata": {
        "id": "fPBjc-gJRLPq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "3ef70d925ee32efc012b20ba64d633497e42b8a12b591679101549b822ba29bf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
