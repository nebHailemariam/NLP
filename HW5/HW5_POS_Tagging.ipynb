{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "459f243c",
   "metadata": {},
   "source": [
    "# 11411/611 – NLP (Spring 23)\n",
    "## HW5 – POS Tagging\n",
    "\n",
    "Deadline: March 14th, 2023 at 11:59pm EST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93764f9",
   "metadata": {},
   "source": [
    "Part of Speech (POS) tagging categorizes words of a sentence, depending on it's definition and the context. For this assignment, we’ve provided you the text of the first 23 sections of the Penn Tree Bank (PTB). The PTB corpus, and in particular the section of the corpus corresponding to the articles of the Wall Street Journal, is one of the most known and used for the evaluation of models for sequence labelling. \n",
    "We’ve also provided you with the gold standard tags of sections 2-21 as well. You can find the list of POS tags for the PTB using this [link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) \n",
    "\n",
    "The first 21 sections are concatenated into `ptb.2-21.txt` and `ptb.2-21.tgs` for convenience. You will use section 22 and 23 `ptb.22.txt`, `ptb.22.tgs`, and `ptb.23.txt` for evaluation purposes in later tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56af9c",
   "metadata": {},
   "source": [
    "ptb datasets (files) refer to the Penn Tree Bank. The .txt files contain the text while the .tgs files contain the POS gold-standard tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec876e5",
   "metadata": {},
   "source": [
    "### Task 1: POS tagging with HMM (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174b64e",
   "metadata": {},
   "source": [
    "In this task you will be using a Hidden Markov Model (HMM) to perform POS tagging. You are constrained to use an HMM, and you can only train on the provided training data (no external resources/libraries allowed). You should train your model with `ptb.2-21.txt` and `ptb.2-21.tgs`, test the model with `ptb.22.txt`, and evaluate your model with `ptb.22.tgs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188f224",
   "metadata": {},
   "source": [
    "#### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0925e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe59057",
   "metadata": {},
   "source": [
    "In this subtask, you will be training your bigram HMM (not to be confused with bigram tokens) using the simple MLE approach introduced in the lecture slides. A bigram HMM is one where the transition probability is defined as: $P(q_i|q_j)$, for $q_i$; $q_j \\in Q$, the set of states, and $S$ is the total number of states. \n",
    "\n",
    "Your implementation should:\n",
    "1. Read in the input file for text $x_1,x_2$,...,$x_N$, the input file for tags, and an output file to write to.\n",
    "2. For every pair of states $q_i$; $q_j \\in Q$, calculate transition probability  $\\gamma _{q_i q_j} = \\frac{count(q_i, q_j)}{\\sum \\limits _{s=1} ^{S} count(q_i, q_s)}$ .\n",
    "You can ignore smoothing for this task. \n",
    "3. For every state $q_i \\in Q$ and every token $x_t$ in the text file, calculate emission probability $\\eta _{q_i x_t} = \\frac{count(q_i, x_t)}{\\sum \\limits _{n=1} ^{N} count(q_i, x_n)} $ .\n",
    "You can ignore smoothing for this task. \n",
    "4. Write the output to an output file.\n",
    "Some of the code has been implemented. You should fill in the sections commented with TO-DO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201460ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMMTrain():\n",
    "    def __init__(self, TAG_FILE, TOKEN_FILE, OUTPUT_FILE):\n",
    "        self.TAG_FILE = TAG_FILE\n",
    "        self.TOKEN_FILE = TOKEN_FILE \n",
    "        self.OUTPUT_FILE = OUTPUT_FILE\n",
    "        #Vocabulary\n",
    "        self.vocab = {}\n",
    "        self.OOV_WORD = \"OOV\"\n",
    "        self.INIT_STATE = \"init\"\n",
    "        self.FINAL_STATE = \"final\"\n",
    "        #Transition and emission probabilities\n",
    "        self.emissions = {} # dictionary to store count of transition from POS tag state to tokens in the corpus\n",
    "        self.transitions = {} # dictionary to store count of transition from previous POS tag state to current state\n",
    "        self.transitions_total = defaultdict(lambda: 0) # total number of transitions from one state to all others\n",
    "        self.emissions_total = defaultdict(lambda: 0) # total number of transitions from one state to all tokens\n",
    "\n",
    "\n",
    "\n",
    "    # train the model\n",
    "    def train(self):\n",
    "        # Read from tag file and token file. \n",
    "        with open(self.TAG_FILE) as tag_file, open(self.TOKEN_FILE) as token_file:\n",
    "            for tag_string, token_string in zip(tag_file, token_file):\n",
    "                tags = re.split(\"\\s+\", tag_string.rstrip())\n",
    "                tokens = re.split(\"\\s+\", token_string.rstrip())\n",
    "                pairs = zip(tags, tokens)\n",
    "\n",
    "                # Starts off with initial state\n",
    "                prevtag = self.INIT_STATE\n",
    "\n",
    "                for (tag, token) in pairs:\n",
    "\n",
    "                    # this block is a little trick to help with out-of-vocabulary (OOV)\n",
    "                    # words.  the first time we see *any* word token, we pretend it\n",
    "                    # is an OOV.  this lets our model decide the rate at which new\n",
    "                    # words of each POS-type should be expected (e.g., high for nouns,\n",
    "                    # low for determiners).\n",
    "\n",
    "                    if token not in self.vocab:\n",
    "                        self.vocab[token] = 1\n",
    "                        token = self.OOV_WORD\n",
    "\n",
    "                    #=======================TO-DO=======================\n",
    "                     \n",
    "                    if(tag not in self.emissions):\n",
    "                        # initialize to store count of transition from 'tag' to the tokens in the corpus\n",
    "                    if(prevtag not in self.transitions):\n",
    "                        # intitialize to store count of transition from 'prevtag' to current tag\n",
    "                    \n",
    "                    #=======================TO-DO=======================\n",
    "                    \n",
    "                    # increment count for self.emissions\n",
    "                    # increment count for self.transitions\n",
    "                    # increment count for self.transitions_total\n",
    "                    # increment count for self.emissions_total\n",
    "                    \n",
    "\n",
    "                # don't forget the stop probability for each sentence\n",
    "                if prevtag not in self.transitions:\n",
    "                    self.transitions[prevtag] = defaultdict(lambda: 0)\n",
    "                \n",
    "                #=======================TO-DO=======================\n",
    "                # increment count for self.transitions from prevtag to final state\n",
    "                # increment count for self.transitions_total for prevtag\n",
    "\n",
    "    #=======================TO-DO=======================\n",
    "    # calculate the transition probability prevtag -> tag\n",
    "    def calculate_transition_prob(self, prevtag, tag):\n",
    "        # TODO: Implement this. You can ignore smoothing in this task.\n",
    "        return 0.0\n",
    "\n",
    "    #=======================TO-DO=======================\n",
    "    #calculate the probability of emitting token given tag\n",
    "    def calculate_emission_prob(self, tag, token):\n",
    "        # TODO: Implement this. You can ignore smoothing in this task.\n",
    "        return 0.0\n",
    "\n",
    "    # Write the model to an output file.\n",
    "    # Doesn't need to be modified\n",
    "    def writeResult(self):\n",
    "        with open(self.OUTPUT_FILE, \"w+\") as f:\n",
    "            for prevtag in self.transitions:\n",
    "                for tag in self.transitions[prevtag]:\n",
    "                    f.write(\"trans {} {} {}\\n\"\n",
    "                        .format(prevtag, tag, self.calculate_transition_prob(prevtag, tag)))\n",
    "\n",
    "            for tag in self.emissions:\n",
    "                for token in self.emissions[tag]:\n",
    "                    f.write(\"emit {} {} {}\\n\"\n",
    "                        .format(tag, token, self.calculate_emission_prob(tag, token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================For testing code(no need to copy into submission file)=======================\n",
    "TAG_FILE = #define this (extension .tgs)\n",
    "TOKEN_FILE = #define this (extension .txt)\n",
    "OUTPUT_FILE = \"model.hmm\"\n",
    "\n",
    "model = HMMTrain(TAG_FILE, TOKEN_FILE, OUTPUT_FILE)\n",
    "model.train()\n",
    "model.writeResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b55d7",
   "metadata": {},
   "source": [
    "`TAG_FILE` is the tag file for training (extension .tgs). `TOKEN_FILE` is the token file for training (extension .txt). `OUTPUT_FILE = model.hmm` is the output file that the above code will write to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fcb873",
   "metadata": {},
   "source": [
    "Below are some test cases to ensure your model is working as intended before you move on to the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(abs(model.calculate_transition_prob(\"verb_past_tense\", \"noun_singular\") - 0.0318749309468567) / 0.0318749309468567 < 0.1)\n",
    "assert(abs(model.calculate_transition_prob(\"init\", \"verb_base_form\") - 0.002803464580107954) / 0.002803464580107954 < 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4358ea04",
   "metadata": {},
   "source": [
    "### Task 2: Implementing the Viterbi Algorithm (60 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17019490",
   "metadata": {},
   "source": [
    "In this subtask, you will be implementing the Viterbi algorithm (which everyone learned in the lecture and remembers well). Your implementation should:\n",
    "1. Read in the HMM model, the input file for text, and an output file to write to.\n",
    "2. Use the Viterbi algorithm to calculate the best pos tag sequence corresponding to each line of the text.\n",
    "3. Write the output to an output file.\n",
    "Some of the code has been implemented. You should fill in the sections commented with TO-DO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fec7f6",
   "metadata": {},
   "source": [
    "**Things to keep in mind:**\n",
    "\n",
    "Input: A string representing a sequence of tokens separated by white spaces <br> Output: A string representing a sequence of POS tags.\n",
    "\n",
    "\n",
    "1. Probability calculations are done in log space. \n",
    "2. Ignore smoothing in this case. For  probabilities of emissions that we haven't seen\n",
    "or  probabilities of transitions that we haven't seen, ignore them. (How to detect them?\n",
    "Remember that values of self.transition and self.emission are default dicts with default \n",
    "value 1.0)\n",
    "3. A word is treated as an OOV word if it has not been seen in the training set. Notice \n",
    "that an unseen token and an unseen transition/emission are different things. You don't \n",
    "have to do any additional thing to handle OOV words.\n",
    "4. There might be cases where your algorithm cannot proceed. For example, you reach a \n",
    "    state that for all prevstate, the transition probability prevstate&rarr;state is unseen, just return an empty string in this case. \n",
    "5. You can set up your Viterbi matrix in many ways (but we all know it's better to implement it with a \n",
    "    python dictionary). For example, you will want to keep track of \n",
    "    the best prevstate that leads to the current state in order to backtrack and find the \n",
    "    best sequence of pos tags. Do you keep track of it in V or do you keep track of it \n",
    "    separately? Up to you!\n",
    "6. Don't forget to handle final state!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047b67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Viterbi():\n",
    "    def __init__(self):\n",
    "        # transition and emission probabilities. Remember that we're not dealing with smoothing \n",
    "        # here. So for the probability of transition and emission of tokens/tags that we haven't \n",
    "        # seen in the training set, we ignore them by setting the probability an impossible value \n",
    "        # of 1.0 (1.0 is impossible because we're in log space)\n",
    "\n",
    "        self.transition = defaultdict(lambda: defaultdict(lambda: 1.0))\n",
    "        self.emission = defaultdict(lambda: defaultdict(lambda: 1.0))\n",
    "        # keep track of states to iterate over \n",
    "        self.states = set()\n",
    "        self.POSStates = set()\n",
    "        # store vocab to check for OOV words\n",
    "        self.vocab = set()\n",
    "\n",
    "        # text to run viterbi with\n",
    "        self.text_file_lines = []\n",
    "        with open(TEXT_FILE, \"r\") as f:\n",
    "            self.text_file_lines = f.readlines()\n",
    "\n",
    "    def readModel(self):\n",
    "        # Read HMM transition and emission probabilities\n",
    "        # Probabilities are converted into LOG SPACE!\n",
    "        with open(HMM_FILE, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.split()\n",
    "\n",
    "                # Read transition\n",
    "                # Example line: trans NN NNPS 9.026968067100463e-05\n",
    "                # Read in states as prev_state -> state\n",
    "                if line[0] == TRANSITION_TAG:\n",
    "                    (prev_state, state, trans_prob) = line[1:4]\n",
    "                    self.transition[prev_state][state] = math.log(float(trans_prob))\n",
    "                    self.states.add(prev_state)\n",
    "                    self.states.add(state)\n",
    "\n",
    "                # Read in states as state -> word\n",
    "                elif line[0] == EMISSION_TAG:\n",
    "                    (state, word, emit_prob) = line[1:4]\n",
    "                    self.emission[state][word] = math.log(float(emit_prob))\n",
    "                    self.states.add(state)\n",
    "                    self.vocab.add(word)\n",
    "\n",
    "        # Keep track of the non-initial and non-final states\n",
    "        self.POSStates = self.states.copy()\n",
    "        self.POSStates.remove(INIT_STATE)\n",
    "        self.POSStates.remove(FINAL_STATE)\n",
    "\n",
    "\n",
    "    # run Viterbi algorithm and write the output to the output file\n",
    "    def runViterbi(self):\n",
    "        result = []\n",
    "        for line in self.text_file_lines:\n",
    "            result.append(self.viterbiLine(line))\n",
    "\n",
    "        # Print output to file\n",
    "        with open(OUTPUT_FILE, \"w\") as f:\n",
    "            for line in result:\n",
    "                f.write(line)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    #=======================TO-DO=======================\n",
    "    def viterbiLine(self, line):\n",
    "        words = line.split()\n",
    "\n",
    "        # Initialize DP matrix for Viterbi here (we suggest using a dictionary)\n",
    "        # You will require a path probability matrix and a backpointer matrix\n",
    "\n",
    "        for (i, word) in enumerate(words):\n",
    "            # replace unseen words as oov\n",
    "            if word not in self.vocab:\n",
    "                word = OOV_WORD\n",
    "\n",
    "            #Fill up your DP matrix here. Remember to handle the init state\n",
    "\n",
    "        #Handle best final state here.\n",
    "\n",
    "        #Backtrack and find the optimal sequence here. \n",
    "\n",
    "\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ec667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================For testing code (no need to copy into submission file)=======================\n",
    "HMM_FILE = \"model.hmm\"\n",
    "TEXT_FILE = # define this (extension .txt)\n",
    "OUTPUT_FILE = \"myoutput.tgs\"\n",
    "TRANSITION_TAG = \"trans\"\n",
    "EMISSION_TAG = \"emit\"\n",
    "OOV_WORD = \"OOV\"         # check that the HMM file uses this same string\n",
    "INIT_STATE = \"init\"      # check that the HMM file uses this same string\n",
    "FINAL_STATE = \"final\"    # check that the HMM file uses this same string\n",
    "\n",
    "t0 = time.time()\n",
    "viterbi = Viterbi()\n",
    "viterbi.readModel()\n",
    "viterbi.runViterbi()\n",
    "# Mark end time\n",
    "t1 = time.time()\n",
    "print(\"Time taken to run: {}\".format(t1 - t0))  # this may take some time to run. Let's be patient!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12f1c4",
   "metadata": {},
   "source": [
    "`HMM_FILE = model.hmm` is the file generated from Subtask 1, `TEXT_FILE` is the file of text tokens (extension .txt), and `OUTPUT_FILE = myoutput.tgs` is the output file to write your pos tags into."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4814b78",
   "metadata": {},
   "source": [
    "### Evaluating your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2251d",
   "metadata": {},
   "source": [
    "Use the code provided below to obtain an accuracy score. `GOLD_TAGS` is the file of correct tags we provide, and `MY_TAGS = myoutput.tgs` is the tags file generated from Subtask 2. Do not modify the code below except for defining your GOLD_TAGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80254b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================For testing (no need to copy into submission file)=======================\n",
    "GOLD_TAGS = #define this (extension .tgs)\n",
    "MY_TAGS = \"myoutput.tgs\"\n",
    "\n",
    "# Stats\n",
    "num_sentences = 0\n",
    "num_sentence_errors = 0\n",
    "num_tokens = 0\n",
    "num_token_errors = 0\n",
    "\n",
    "# Iterate over both files\n",
    "gold_tags = []\n",
    "with open(GOLD_TAGS, \"r\") as gold_tags, open(MY_TAGS, \"r\") as my_tags:\n",
    "\n",
    "    # zip_longest allows us to iterate over the length of the longer list\n",
    "    for (gold_tag_line, my_tag_line) in zip_longest(gold_tags, my_tags):\n",
    "\n",
    "        # Terminate loop if more lines in my_tags than gold_tags\n",
    "        if not gold_tag_line:\n",
    "            break\n",
    "\n",
    "        # If missing line, add entire missing line to error num stats\n",
    "        num_sentences += 1\n",
    "        if not my_tag_line:\n",
    "            num_sentence_errors += 1\n",
    "            gold_tag = re.split(\"\\s+\", gold_tags.rstrip())            \n",
    "            num_tokens += len(gold_tag)\n",
    "            num_token_errors += len(gold_tag)\n",
    "            continue\n",
    "\n",
    "        # Otherwise, compare both lines token by token\n",
    "        sentence_errors = 0\n",
    "        for (gold_tag, my_tag) in zip_longest(re.split(\"\\s+\", gold_tag_line.rstrip()), re.split(\"\\s+\", my_tag_line.rstrip())):\n",
    "\n",
    "            # Terminate line if my_tag_line longer than gold_tag_line\n",
    "            if not gold_tag:\n",
    "                break\n",
    "\n",
    "            num_tokens += 1\n",
    "            if gold_tag != my_tag:\n",
    "                num_token_errors += 1\n",
    "                sentence_errors += 1\n",
    "\n",
    "        if sentence_errors > 0:\n",
    "            num_sentence_errors += 1\n",
    "\n",
    "# Print stats\n",
    "print(\"Accuracy by word: {}\".format(1 - num_token_errors / num_tokens))\n",
    "print(\"Accuracy by sentence: {}\".format(1 - num_sentence_errors / num_sentences))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87d652c7",
   "metadata": {},
   "source": [
    "### Task 3: Building a Better System (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dcea025",
   "metadata": {},
   "source": [
    "Woohoo! You have implemented a bigram HMM model! Clearly we can do better. Now it is time to improve the model. You are still constrained to using a HMM and training on the provided training data (no external resources/libraries allowed). Some relatively simple but cost-effective measures include implementing a trigram HMM, or smoothing the probability estimates. Here we are going to implement Kneser-Ney smoothing. Feel free to use multiple approaches to improve the model. Measure the accuracy of your new model against the same dataset as in the previous task. Once you feel satisfied with the improvements, run and test your shiny new tagger on `ptb.22.txt` and copy this code in train_hmm_smoothed.py."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b796fd5",
   "metadata": {},
   "source": [
    "Your improvements may reduce the throughput of your tagging program by an order of magnitude. While we do not grade you on your code efficiency or speed, in the interest of your personal productivity, consider sampling a smaller but appropriate amount of data to test your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a2c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMMTrainSmoothed():\n",
    "    def __init__(self, TAG_FILE, TOKEN_FILE, OUTPUT_FILE):\n",
    "        self.TAG_FILE = TAG_FILE\n",
    "        self.TOKEN_FILE = TOKEN_FILE \n",
    "        self.OUTPUT_FILE = OUTPUT_FILE\n",
    "        #Vocabulary\n",
    "        self.vocab = {}\n",
    "        self.OOV_WORD = \"OOV\"\n",
    "        self.INIT_STATE = \"init\"\n",
    "        self.FINAL_STATE = \"final\"\n",
    "        #Transition and emission probabilities\n",
    "        self.emissions = {} # dictionary to store count of transition from POS tag state to tokens in the corpus\n",
    "        self.transitions = {} # dictionary to store count of transition from previous POS tag state to current state\n",
    "        self.transitions_total = defaultdict(lambda: 0) # total number of transitions from one state to all others\n",
    "        self.emissions_total = defaultdict(lambda: 0) # total number of transitions from one state to all tokens\n",
    "        self.count_of_transitions = 0\n",
    "        self.count_of_emissions = 0\n",
    "        self.d = 0.75 # Discount factor\n",
    "\n",
    "\n",
    "    # train the model\n",
    "    def train(self):\n",
    "        # Read from tag file and token file. \n",
    "        with open(self.TAG_FILE) as tag_file, open(self.TOKEN_FILE) as token_file:\n",
    "            for tag_string, token_string in zip(tag_file, token_file):\n",
    "                tags = re.split(\"\\s+\", tag_string.rstrip())\n",
    "                tokens = re.split(\"\\s+\", token_string.rstrip())\n",
    "                pairs = zip(tags, tokens)\n",
    "\n",
    "                # Starts off with initial state\n",
    "                prevtag = self.INIT_STATE\n",
    "\n",
    "                for (tag, token) in pairs:\n",
    "\n",
    "                    # this block is a little trick to help with out-of-vocabulary (OOV)\n",
    "                    # words.  the first time we see *any* word token, we pretend it\n",
    "                    # is an OOV.  this lets our model decide the rate at which new\n",
    "                    # words of each POS-type should be expected (e.g., high for nouns,\n",
    "                    # low for determiners).\n",
    "\n",
    "                    if token not in self.vocab:\n",
    "                        self.vocab[token] = 1\n",
    "                        token = self.OOV_WORD\n",
    "\n",
    "                    #=======================TO-DO=======================\n",
    "                     \n",
    "                    if(tag not in self.emissions):\n",
    "                        # initialize to store count of transition from 'tag' to the tokens in the corpus\n",
    "                    if(prevtag not in self.transitions):\n",
    "                        # intitialize to store count of transition from 'prevtag' to current tag\n",
    "                    \n",
    "                    #=======================TO-DO=======================\n",
    "                    \n",
    "                    # increment count for self.emissions\n",
    "                    # increment count for self.transitions\n",
    "                    # increment count for self.transitions_total\n",
    "                    # increment count for self.emissions_total\n",
    "                    \n",
    "\n",
    "                # don't forget the stop probability for each sentence\n",
    "                if prevtag not in self.transitions:\n",
    "                    self.transitions[prevtag] = defaultdict(lambda: 0)\n",
    "                \n",
    "                #=======================TO-DO=======================\n",
    "                # increment count for self.transitions from prevtag to final state\n",
    "                # increment count for self.transitions_total for prevtag\n",
    "                # Count the total number of unique transitions for count_of_transitions\n",
    "                # Count the total number of unique emissions for count_of_emissions\n",
    "            \n",
    "\n",
    "    #=======================TO-DO=======================\n",
    "    # calculate the transition probability prevtag -> tag\n",
    "    def calculate_transition_prob(self, prevtag, tag):\n",
    "        \n",
    "        # Discount factor is defined in init\n",
    "        \n",
    "        # Calculate the continuation probability using the 'novel continuation for a tag' (number of word types seen to precede tag) and the total number of bigram types\n",
    "        # You can choose to fill the template for the function calculate_novel_continuation and use it or you can do write your own code here.\n",
    "        \n",
    "        \n",
    "        # Calculate the backoff probability using the count of the transitions with prevtag and the number of unique words than can follow prevtag\n",
    "        \n",
    "        \n",
    "        # Calculate the smoothed probability\n",
    "        \n",
    "        return 0.0\n",
    "\n",
    "    #=======================TO-DO=======================\n",
    "    #calculate the probability of emitting token given tag\n",
    "    def calculate_emission_prob(self, tag, token):\n",
    "        \n",
    "        # Calculate the continuation probability using the 'novel continuation for a token' (number of tags pointing to token) and the total number of emissions\n",
    "        # You can choose to fill the template for the function calculate_novel_continuation and use it or you can do write your own code here.\n",
    "        \n",
    "        \n",
    "        # Calculate the backoff probability using the count of the emissions from tag and the number of unique tokens than can be emitted from tags\n",
    "        \n",
    "        \n",
    "        # Calculate the smoothed probability\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def calculate_novel_continuation(self, tag_or_token, transition = True):\n",
    "        # If transition is true, this function was called by calculate_transition_prob otherwise calculate_emission_prob\n",
    "        main_checker = self.transitions\n",
    "        if transition == False:\n",
    "            main_checker = self.emissions\n",
    "        \n",
    "        # Loop on main_checker to either:\n",
    "        # 1. For transitions, get the number of all prevtags which have tag as a transition\n",
    "        # or, 2. For emission, get the number of all tags which have token emitting from it and return it.\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "\n",
    "    # Write the model to an output file.\n",
    "    # Doesn't need to be modified\n",
    "    def writeResult(self):\n",
    "        with open(self.OUTPUT_FILE, \"w+\") as f:\n",
    "            for prevtag in self.transitions:\n",
    "                for tag in self.transitions[prevtag]:\n",
    "                    f.write(\"trans {} {} {}\\n\"\n",
    "                        .format(prevtag, tag, self.calculate_transition_prob(prevtag, tag)))\n",
    "\n",
    "            for tag in self.emissions:\n",
    "                for token in self.emissions[tag]:\n",
    "                    f.write(\"emit {} {} {}\\n\"\n",
    "                        .format(tag, token, self.calculate_emission_prob(tag, token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa48d1a",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25488973",
   "metadata": {},
   "source": [
    "You are required to submit `train_hmm.py` and `viterbi.py`. Copy and paste in the code from Subtask 1 into `train_hmm.py` and the code from Subtask 2 into `viterbi.py`. Your program will be run using the following format.\n",
    "\n",
    "`python train_hmm.py train_file.tgs train_file.txt model.hmm`\n",
    "\n",
    "`python viterbi.py model.hmm input.txt myoutput.tgs`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9434697081d0ba3380ae207032ef0a6b094824a110fbfe9d4d3ac7077e4952c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
