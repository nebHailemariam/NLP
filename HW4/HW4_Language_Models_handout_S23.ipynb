{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "25d624c1",
      "metadata": {
        "id": "25d624c1"
      },
      "source": [
        "# 11411/611 – NLP (S23)\n",
        "\n",
        "\n",
        "## HW3 – Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab1cb6a9",
      "metadata": {
        "id": "ab1cb6a9"
      },
      "source": [
        "**File version:** 1.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8710365b",
      "metadata": {
        "id": "8710365b"
      },
      "source": [
        "Whether for transcribing spoken utterances as correct word sequences or generating coherent human-like text, language models are extremely useful.\n",
        "\n",
        "In this assignment, you will be building your own language model powered by n-grams."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9cda894",
      "metadata": {
        "id": "d9cda894"
      },
      "source": [
        "### There are two major components in this HW:\n",
        "#### Part 1: Programming [60 marks]\n",
        "You are required to program an n-gram language model.\n",
        "\n",
        "#### Part 2: Analyses [40 marks]\n",
        "After writing the code, you are required to answer the empirical questions in the handout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e485bc65",
      "metadata": {
        "id": "e485bc65"
      },
      "source": [
        "### Submission Guidelines\n",
        "\n",
        "**Deadline:** February 24th, 2023 at 11:59pm EST"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f727d4",
      "metadata": {
        "id": "e9f727d4"
      },
      "source": [
        "**Programming:** \n",
        "- This notebook contains helpful test cases and additional information about the programming part of the HW. However, you are only required to submit `lm.py` and `utils.py` on Gradescope.\n",
        "- Remember to upload the data.zip file provided as part of the handout\n",
        "- We recommended that you first code in the notebook and then copy the corresponding methods/classes to `lm.py`.\n",
        "\n",
        "**Written:**\n",
        "- Analyses questions would require you to run your code.\n",
        "- You need to write your answers in a document and upload it alongside the programming components\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wjvLk7XPMxnl",
      "metadata": {
        "id": "wjvLk7XPMxnl"
      },
      "source": [
        "### Upload main.py and utils.py, and the data.zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bmEQByNzNI4d",
      "metadata": {
        "id": "bmEQByNzNI4d"
      },
      "outputs": [],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f84134d",
      "metadata": {
        "id": "1f84134d"
      },
      "source": [
        "## Part 1: Language Models [60 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0b3c6b",
      "metadata": {
        "id": "4d0b3c6b"
      },
      "source": [
        "### Step 0: Importing essential libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30667bf4",
      "metadata": {
        "id": "30667bf4"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from itertools import product\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d5ca5b6",
      "metadata": {
        "id": "3d5ca5b6"
      },
      "source": [
        "### Step 1: Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a6eedf",
      "metadata": {
        "id": "b3a6eedf"
      },
      "source": [
        "We provide you with a few functions in `utils.py` to read and preprocess your input data. Do not edit this file!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "195db6a3",
      "metadata": {
        "id": "195db6a3"
      },
      "outputs": [],
      "source": [
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ea47b0",
      "metadata": {
        "id": "f4ea47b0"
      },
      "source": [
        "We have performed a round of preprocessing on the datasets.\n",
        "\n",
        "- Each file contains one sentence per line.\n",
        "- All punctuation marks have been removed.\n",
        "- Each line is a sequences of tokens separated by whitespace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fbe98cc",
      "metadata": {
        "id": "5fbe98cc"
      },
      "source": [
        "#### Special Symbols ( Already defined in `utils.py` )\n",
        "The start and end tokens will act as padding to the given sentences, to make sure they are correctly defined, print them here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed9e54f",
      "metadata": {
        "id": "9ed9e54f"
      },
      "outputs": [],
      "source": [
        "print(\"Sentence START symbol: {}\".format(START))\n",
        "print(\"Sentence END symbol: {}\".format(EOS))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b1f4a6f",
      "metadata": {
        "id": "6b1f4a6f"
      },
      "source": [
        "#### Reading and processing an example file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60ce7c2",
      "metadata": {
        "id": "d60ce7c2"
      },
      "outputs": [],
      "source": [
        "# Read the sample file\n",
        "sample = read_file(\"data/sample.txt\")\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec373cc",
      "metadata": {
        "id": "4ec373cc"
      },
      "outputs": [],
      "source": [
        "# Preprocess the content to add corresponding number of start and end tokens. Try out the method with n = 3 and n = 4 as well.\n",
        "# Preprocessing example for bigrams (n=2)\n",
        "sample = preprocess(sample, n=2)\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d264145e",
      "metadata": {
        "id": "d264145e"
      },
      "source": [
        "### Step 2: Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1322297b",
      "metadata": {
        "id": "1322297b"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "def flatten(lst):\n",
        "    \"\"\"\n",
        "    Flattens a nested list into a 1D list.\n",
        "    Args:\n",
        "        lst: Nested list (2D)\n",
        "    \n",
        "    Returns:\n",
        "        Flattened 1-D list\n",
        "    \"\"\"\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RaVZlauerDkn",
      "metadata": {
        "id": "RaVZlauerDkn"
      },
      "outputs": [],
      "source": [
        "\n",
        "assert flatten([[\"a\", \"b\", \"c\"], [\"d\"]]) == ['a', 'b', 'c', 'd']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9165b404",
      "metadata": {
        "id": "9165b404"
      },
      "source": [
        "### Step 3: Get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4f73b0",
      "metadata": {
        "id": "5a4f73b0"
      },
      "source": [
        "### TO DO: `get_ngrams()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "138c35b6",
      "metadata": {
        "id": "138c35b6"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TO-DO: get_ngrams()\n",
        "#######################################\n",
        "def get_ngrams(list_of_words, n):\n",
        "    \"\"\"\n",
        "    Returns a list of n-grams for a list of words.\n",
        "    Args\n",
        "    ----\n",
        "    list_of_words: List[str]\n",
        "        List of already preprocessed and flattened (1D) list of tokens e.g. [\"<s>\", \"hello\", \"</s>\", \"<s>\", \"bye\", \"</s>\"]\n",
        "    n: int\n",
        "        n-gram order e.g. 1, 2, 3\n",
        "    \n",
        "    Returns:\n",
        "        n_grams: List[Tuple]\n",
        "            Returns a list containing n-gram tuples\n",
        "    \"\"\"\n",
        "\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fdab35a",
      "metadata": {
        "id": "5fdab35a"
      },
      "outputs": [],
      "source": [
        "sample = read_file(\"data/sample.txt\")\n",
        "sample = preprocess(sample, n=3)\n",
        "\n",
        "assert get_ngrams(flatten(sample), 3) == [('<s>', '<s>', 'we'),\n",
        " ('<s>', 'we', 'are'),\n",
        " ('we', 'are', 'never'),\n",
        " ('are', 'never', 'ever'),\n",
        " ('never', 'ever', 'ever'),\n",
        " ('ever', 'ever', 'ever'),\n",
        " ('ever', 'ever', 'ever'),\n",
        " ('ever', 'ever', 'getting'),\n",
        " ('ever', 'getting', 'back'),\n",
        " ('getting', 'back', 'together'),\n",
        " ('back', 'together', '</s>'),\n",
        " ('together', '</s>', '<s>'),\n",
        " ('</s>', '<s>', '<s>'),\n",
        " ('<s>', '<s>', 'we'),\n",
        " ('<s>', 'we', 'are'),\n",
        " ('we', 'are', 'the'),\n",
        " ('are', 'the', 'ones'),\n",
        " ('the', 'ones', 'together'),\n",
        " ('ones', 'together', 'we'),\n",
        " ('together', 'we', 'are'),\n",
        " ('we', 'are', 'back'),\n",
        " ('are', 'back', '</s>')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bbb4a88",
      "metadata": {
        "id": "7bbb4a88"
      },
      "source": [
        "### **TO DO:** Class `LanguageModel()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7f174c9",
      "metadata": {
        "id": "f7f174c9"
      },
      "source": [
        "*Now*, we will define our LanguageModel class.\n",
        "\n",
        "**Some Useful Variables:**\n",
        "- self.model: `dict` of n-grams and their corresponding probabilities, key's being the tuple containing the n-gram, and the value being the probability of the n-gram.\n",
        "- self.vocab: `dict` of unigram vocabulary with counts, key's being the words themselves and the values being their frequency.\n",
        "- self.n: `int` value for n-gram order (e.g. 1,2,3).\n",
        "- self.train_data: `List[List]` containing preprocessed **unflattened** train sentences. You will have to flatten it to use in the Language Model\n",
        "- self.smoothing: `float` flag signifying the smoothing parameter (was called `k` in the previous HW).\n",
        "\n",
        "In `lm.py`, we will be taking most of these argumemts from command line using this command:\n",
        "\n",
        "`python3 lm.py --train data/sample.txt --test data/sample.txt --n 3 --smoothing 0 --min_freq 1`\n",
        "\n",
        "Note that we will not be using Log probabilities in this section. Store the probabilities as they are, not in the log space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wmXiYmTlw9lS",
      "metadata": {
        "id": "wmXiYmTlw9lS"
      },
      "source": [
        "## Laplace Smoothing\n",
        "\n",
        "There's 2 ways to perform this:\n",
        "- Either you calculate all possible n-gram at train time and calculate smooth probabilities for all of them, hence inflating the model (Eager Smoothing). You then use the probabilities as when required at test time. **OR**\n",
        "- You calculate the probabilities for the **observed n-grams** at train time, using the smoothed likelihood formula, then if any unseen n-gram is observed at test time, you calculate the probability using the smoothed likelihood formula and store it in the model for future use. (Lazy Smoothing)\n",
        "\n",
        "You will be implementing Lazy Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f2ce5c",
      "metadata": {
        "id": "24f2ce5c"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TO-DO: LanguageModel()\n",
        "#######################################\n",
        "class LanguageModel():\n",
        "    def __init__(self, n, train_data, alpha=1):\n",
        "        \"\"\"\n",
        "        Language model class.\n",
        "        \n",
        "        Args\n",
        "        ____\n",
        "        n: int\n",
        "            n-gram order\n",
        "        train_data: List[List]\n",
        "            already preprocessed unflattened list of sentences. e.g. [[\"<s>\", \"hello\", \"my\", \"</s>\"], [\"<s>\", \"hi\", \"there\", \"</s>\"]]\n",
        "        alpha: float\n",
        "            Smoothing parameter\n",
        "        \n",
        "        Other attributes:\n",
        "            self.tokens: list of individual tokens present in the training corpus\n",
        "            self.vocab: vocabulary dict with counts\n",
        "            self.model: n-gram language model, i.e., n-gram dict with probabilties\n",
        "            self.n_grams_counts: dictionary for storing the frequency of ngrams in the training data, keys being the tuple of words(n-grams) and value being their frequency\n",
        "            self.prefix_counts: dictionary for storing the frequency of the (n-1) grams in the data, similar to the self.n_grams_counts\n",
        "            As an example:\n",
        "            For a trigram model, the n-gram would be (w1,w2,w3), the corresponding [n-1] gram would be (w1,w2)\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.train_data = train_data\n",
        "        self.n_grams_counts = {}\n",
        "        self.prefix_counts = {}\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        # Fill in the following two lines of code\n",
        "        self.tokens = #TODO\n",
        "        self.vocab  = #TODO\n",
        "\n",
        "        self.model = self.build()\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Returns a n-gram dict with their smoothed probabilities. Remember to consider the edge case of n=1 as well\n",
        "        \n",
        "        You are expected to update the self.n_grams_counts and self.prefix_counts, and use those calculate the probabilities. \n",
        "        \"\"\"\n",
        "        # Extract n-grams from the flattened training data [update n_grams_counts]\n",
        "        \n",
        "        # Calculate the prefix (n-1 grams) count using the extracted n-grams [update prefix_counts]\n",
        "        \n",
        "        # Calculate probabilities using the get_smooth_probabilities function, you need to define the function\n",
        "\n",
        "        # Return the probabilities\n",
        "        \n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def get_smooth_probabilites(self,n_gram):\n",
        "        \"\"\"\n",
        "        Returns the smoothed probability of the n-gram, using Laplace Smoothing. \n",
        "        Remember to consider the edge case of  n = 1\n",
        "        HINT: Use self.n_gram_counts, self.tokens and self.prefix_counts \n",
        "        \"\"\"\n",
        "        \n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909b9c4a",
      "metadata": {
        "id": "909b9c4a"
      },
      "outputs": [],
      "source": [
        "# Quick test\n",
        "sample = read_file(\"data/sample.txt\")\n",
        "sample = preprocess(sample, n=2)\n",
        "# For the sake of understanding we will pass alpha as 0 (No smoothing), so that you gain intuition about the probabilities\n",
        "test_lm = LanguageModel(n=2, train_data=sample, alpha=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "501ac225",
      "metadata": {
        "id": "501ac225"
      },
      "outputs": [],
      "source": [
        "assert test_lm.vocab == Counter({'<s>': 2,\n",
        "         'we': 3,\n",
        "         'are': 3,\n",
        "         'never': 1,\n",
        "         'ever': 4,\n",
        "         'getting': 1,\n",
        "         'back': 2,\n",
        "         'together': 2,\n",
        "         '</s>': 2,\n",
        "         'the': 1,\n",
        "         'ones': 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "157b0749",
      "metadata": {
        "id": "157b0749"
      },
      "outputs": [],
      "source": [
        "assert test_lm.model =={('<s>', 'we'): 1.0,\n",
        " ('we', 'are'): 1.0,\n",
        " ('are', 'never'): 0.3333333333333333,\n",
        " ('never', 'ever'): 1.0,\n",
        " ('ever', 'ever'): 0.75,\n",
        " ('ever', 'getting'): 0.25,\n",
        " ('getting', 'back'): 1.0,\n",
        " ('back', 'together'): 0.5,\n",
        " ('together', '</s>'): 0.5,\n",
        " ('</s>', '<s>'): 1.0,\n",
        " ('are', 'the'): 0.3333333333333333,\n",
        " ('the', 'ones'): 1.0,\n",
        " ('ones', 'together'): 1.0,\n",
        " ('together', 'we'): 0.5,\n",
        " ('are', 'back'): 0.3333333333333333,\n",
        " ('back', '</s>'): 0.5}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yUkFFVNsz-Vo",
      "metadata": {
        "id": "yUkFFVNsz-Vo"
      },
      "source": [
        "Let's see what happens when we add smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t40TXO0M0DDs",
      "metadata": {
        "id": "t40TXO0M0DDs"
      },
      "outputs": [],
      "source": [
        "sample = read_file(\"data/sample.txt\")\n",
        "sample = preprocess(sample, n=2)\n",
        "test_lm = LanguageModel(n=2, train_data=sample, alpha=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wi3JnWaW0Ley",
      "metadata": {
        "id": "wi3JnWaW0Ley"
      },
      "outputs": [],
      "source": [
        "assert test_lm.vocab == Counter({'<s>': 2,\n",
        "         'we': 3,\n",
        "         'are': 3,\n",
        "         'never': 1,\n",
        "         'ever': 4,\n",
        "         'getting': 1,\n",
        "         'back': 2,\n",
        "         'together': 2,\n",
        "         '</s>': 2,\n",
        "         'the': 1,\n",
        "         'ones': 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LAMBhtxf0LVq",
      "metadata": {
        "id": "LAMBhtxf0LVq"
      },
      "outputs": [],
      "source": [
        "assert test_lm.model =={('<s>', 'we'): 0.23076923076923078,\n",
        " ('we', 'are'): 0.2857142857142857,\n",
        " ('are', 'never'): 0.14285714285714285,\n",
        " ('never', 'ever'): 0.16666666666666666,\n",
        " ('ever', 'ever'): 0.26666666666666666,\n",
        " ('ever', 'getting'): 0.13333333333333333,\n",
        " ('getting', 'back'): 0.16666666666666666,\n",
        " ('back', 'together'): 0.15384615384615385,\n",
        " ('together', '</s>'): 0.15384615384615385,\n",
        " ('</s>', '<s>'): 0.16666666666666666,\n",
        " ('are', 'the'): 0.14285714285714285,\n",
        " ('the', 'ones'): 0.16666666666666666,\n",
        " ('ones', 'together'): 0.16666666666666666,\n",
        " ('together', 'we'): 0.15384615384615385,\n",
        " ('are', 'back'): 0.14285714285714285,\n",
        " ('back', '</s>'): 0.15384615384615385}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Unigram language model"
      ],
      "metadata": {
        "id": "z3drL7Mf7iVs"
      },
      "id": "z3drL7Mf7iVs"
    },
    {
      "cell_type": "code",
      "source": [
        "sample = read_file(\"data/sample.txt\")\n",
        "sample = preprocess(sample, n=1)\n",
        "test_lm = LanguageModel(n=1, train_data=sample, alpha=1)"
      ],
      "metadata": {
        "id": "m1G7xqP67h8L"
      },
      "id": "m1G7xqP67h8L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_lm.vocab == Counter({'<s>': 2,\n",
        "         'we': 3,\n",
        "         'are': 3,\n",
        "         'never': 1,\n",
        "         'ever': 4,\n",
        "         'getting': 1,\n",
        "         'back': 2,\n",
        "         'together': 2,\n",
        "         '</s>': 2,\n",
        "         'the': 1,\n",
        "         'ones': 1})"
      ],
      "metadata": {
        "id": "X2QPpMFt7o2_"
      },
      "id": "X2QPpMFt7o2_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_lm.model == {('<s>',): 0.09090909090909091,\n",
        " ('we',): 0.12121212121212122,\n",
        " ('are',): 0.12121212121212122,\n",
        " ('never',): 0.06060606060606061,\n",
        " ('ever',): 0.15151515151515152,\n",
        " ('getting',): 0.06060606060606061,\n",
        " ('back',): 0.09090909090909091,\n",
        " ('together',): 0.09090909090909091,\n",
        " ('</s>',): 0.09090909090909091,\n",
        " ('the',): 0.06060606060606061,\n",
        " ('ones',): 0.06060606060606061}"
      ],
      "metadata": {
        "id": "qWXt-53f7u_n"
      },
      "id": "qWXt-53f7u_n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "edd7f21f",
      "metadata": {
        "id": "edd7f21f"
      },
      "source": [
        "### **TO DO:**  `perplexity()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lD1vQRvF19hU",
      "metadata": {
        "id": "lD1vQRvF19hU"
      },
      "source": [
        "## **TODO**: Perplexity()\n",
        "Steps:\n",
        "1. Flatten the test data\n",
        "2. Extract ngrams from the flattened data\n",
        "3. Calculate perplexity according to given formula. For unseen n-gram, calculate using smoothed likelihood  and store the unseen n-gram probability in the labguage model `model` attribute\n",
        "\n",
        "$PP(W_{test}) = PP(W_1W_2 ... W_n)^{-1/n} $\n",
        "\n",
        "Tips:\n",
        "- Remember that product changes summation under `log`. Take log of probabilities (`math.log()`), sum them up (`sum()`) and then exponentiate it (`math.exp()`) to get back to the original scale.\n",
        "- Make sure to `flatten()` your data before creating the n_grams using `get_ngrams().`\n",
        "- The test suite provided is **not exhaustive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041d12b1",
      "metadata": {
        "id": "041d12b1"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TO-DO: perplexity()\n",
        "#######################################\n",
        "def perplexity(lm, test_data):\n",
        "    \"\"\"\n",
        "    Returns perplexity calculated on the test data.\n",
        "    Args\n",
        "    ----------\n",
        "    test_data: List[List] \n",
        "        Already preprocessed nested list of sentences\n",
        "        \n",
        "    lm: LanguageModel class object\n",
        "        To be used for retrieving lm.model, lm.n and lm.vocab\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Calculated perplexity value\n",
        "    \"\"\"\n",
        "    \n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2927e9aa",
      "metadata": {
        "id": "2927e9aa"
      },
      "outputs": [],
      "source": [
        "# Quick test\n",
        "test_lm = LanguageModel(n=2, train_data=sample, alpha=0)\n",
        "test_ppl = perplexity(test_lm, sample)\n",
        "assert test_ppl < 1.7\n",
        "assert test_ppl > 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eSQa_kV3ZLw",
      "metadata": {
        "id": "7eSQa_kV3ZLw"
      },
      "outputs": [],
      "source": [
        "test_lm = LanguageModel(n=2, train_data=sample, alpha=1)\n",
        "test_ppl = perplexity(test_lm, sample)\n",
        "assert test_ppl < 5.0\n",
        "assert test_ppl > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0212dc6c",
      "metadata": {
        "id": "0212dc6c"
      },
      "source": [
        "### Step 4: Bringing everything together!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35874f48",
      "metadata": {
        "id": "35874f48"
      },
      "source": [
        "**Note:** Most of these will already be defined for you in `main()` method in `lm.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0fcc1c",
      "metadata": {
        "id": "fe0fcc1c"
      },
      "outputs": [],
      "source": [
        "# Arguments\n",
        "\n",
        "train_path = \"data/bbc/tech.txt\"\n",
        "test_path = \"data/bbc/tech.txt\"\n",
        "n = 3\n",
        "min_freq = 1\n",
        "smoothing = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c59236d",
      "metadata": {
        "id": "3c59236d"
      },
      "outputs": [],
      "source": [
        "\n",
        "train = read_file(train_path)\n",
        "test = read_file(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c4ad51",
      "metadata": {
        "id": "b1c4ad51"
      },
      "outputs": [],
      "source": [
        "print(\"No of sentences in train file: {}\".format(len(train)))\n",
        "print(\"No of sentences in test file: {}\".format(len(test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "271bd85a",
      "metadata": {
        "id": "271bd85a"
      },
      "outputs": [],
      "source": [
        "print(\"Raw train example: \\n{}\".format(train[2]))\n",
        "print(\"Raw test example: \\n{}\".format(test[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9edfdb7",
      "metadata": {
        "id": "d9edfdb7"
      },
      "outputs": [],
      "source": [
        "# Basic preprocessing\n",
        "train = preprocess(train, n)\n",
        "test = preprocess(test, n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f0a63a6",
      "metadata": {
        "id": "9f0a63a6"
      },
      "outputs": [],
      "source": [
        "print(\"Preprocessed train example: \\n{}\\n\".format(train[2]))\n",
        "print(\"Preprocessed test example: \\n{}\".format(test[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a85f6b6",
      "metadata": {
        "id": "4a85f6b6"
      },
      "outputs": [],
      "source": [
        "print(\"Loading {}-gram model.\".format(n))\n",
        "lm = LanguageModel(n, train, smoothing)\n",
        "print(\"Vocabulary size (unique unigrams): {}\".format(len(lm.vocab)))\n",
        "print(\"Total number of unique n-grams: {}\".format(len(lm.model)))\n",
        "\n",
        "assert len(lm.vocab) == 12133\n",
        "assert len(lm.model) == 145967"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f11d60",
      "metadata": {
        "id": "22f11d60"
      },
      "outputs": [],
      "source": [
        "# Calculate perplexity\n",
        "\n",
        "ppl = perplexity(lm, test)\n",
        "print(\"Model perplexity: {:.3f}\".format(ppl))\n",
        "assert ppl <= 443.00"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfab26d5",
      "metadata": {
        "id": "bfab26d5"
      },
      "source": [
        "### Step 5: Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "057decc9",
      "metadata": {
        "id": "057decc9"
      },
      "source": [
        "**Note**: These methods are already written for you. Use them to solve Written subtasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2289b988",
      "metadata": {
        "id": "2289b988"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(11411)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2382a8",
      "metadata": {
        "id": "ee2382a8"
      },
      "outputs": [],
      "source": [
        "def best_candidate(lm, prev, i, without=[], mode=\"random\"):\n",
        "    \"\"\"\n",
        "    Returns the most probable word candidate after a given sentence.\n",
        "    \"\"\"\n",
        "    blacklist  = [\"<UNK>\"] + without\n",
        "    candidates = ((ngram[-1],prob) for ngram,prob in lm.model.items() if ngram[:-1]==prev)\n",
        "    candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
        "    candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
        "    if len(candidates) == 0:\n",
        "        return (\"</s>\", 1)\n",
        "    else:\n",
        "        if(mode==\"random\"):\n",
        "            return candidates[random.randrange(len(candidates))]\n",
        "        else:\n",
        "            return candidates[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1c51278",
      "metadata": {
        "id": "f1c51278"
      },
      "outputs": [],
      "source": [
        "def top_k_best_candidates(lm, prev, k, without=[]):\n",
        "    \"\"\"\n",
        "    Returns the K most-probable word candidate after a given n-1 gram.\n",
        "    Args\n",
        "    ----\n",
        "    lm: LanguageModel class object\n",
        "    \n",
        "    prev: n-1 gram\n",
        "        List of tokens n\n",
        "    \"\"\"\n",
        "    blacklist  = [\"<UNK>\"] + without\n",
        "    candidates = ((ngram[-1],prob) for ngram,prob in lm.model.items() if ngram[:-1]==prev)\n",
        "    candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
        "    candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
        "    if len(candidates) == 0:\n",
        "        return (\"</s>\", 1)\n",
        "    else:\n",
        "        return candidates[:k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe5444d",
      "metadata": {
        "id": "9fe5444d"
      },
      "outputs": [],
      "source": [
        "def generate_sentences_from_phrase(lm, num, sent, prob, mode):\n",
        "    \"\"\"\n",
        "    Generate sentences using the trained language model.\n",
        "    \"\"\"\n",
        "    min_len=12\n",
        "    max_len=24\n",
        "    \n",
        "    for i in range(num):\n",
        "        while sent[-1] != \"</s>\":\n",
        "            prev = () if lm.n == 1 else tuple(sent[-(lm.n-1):])\n",
        "            blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
        "\n",
        "            next_token, next_prob = best_candidate(lm, prev, i, without=blacklist, mode=mode)\n",
        "            sent.append(next_token)\n",
        "            prob *= next_prob\n",
        "            \n",
        "            if len(sent) >= max_len:\n",
        "                sent.append(\"</s>\")\n",
        "\n",
        "        yield ' '.join(sent), -1/math.log(prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14ba8069",
      "metadata": {
        "id": "14ba8069"
      },
      "source": [
        "## Part 2: Written [40 points]. We have given some code for some of the written parts to make it easier for you."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53431996",
      "metadata": {
        "id": "53431996"
      },
      "source": [
        "### **Written 4.2** – Song Attribution [8 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4751ea5b",
      "metadata": {
        "id": "4751ea5b"
      },
      "outputs": [],
      "source": [
        "# Example code for Taylor Swift LM\n",
        "n = 3\n",
        "smoothing = 0.1\n",
        "min_freq = 1\n",
        "\n",
        "train = read_file(\"data/lyrics/taylor_swift.txt\")\n",
        "test = read_file(\"data/lyrics/test_lyrics.txt\")\n",
        "\n",
        "train = preprocess(train, n)\n",
        "test = preprocess(test, n)\n",
        "lm = LanguageModel(n, train, smoothing)\n",
        "\n",
        "ppl = perplexity(lm, test)\n",
        "print(ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "749a49a0",
      "metadata": {
        "id": "749a49a0"
      },
      "source": [
        "### **Written 4.3.1** –  Intro to Decoding [8 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ed9b85",
      "metadata": {
        "id": "c8ed9b85"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "smoothing = 0.1\n",
        "min_freq = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96b3d000",
      "metadata": {
        "id": "96b3d000"
      },
      "outputs": [],
      "source": [
        "train = read_file(\"data/bbc/entertainment.txt\")\n",
        "train = preprocess(train, n)\n",
        "lm = LanguageModel(n, train, smoothing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17794ab7",
      "metadata": {
        "id": "17794ab7"
      },
      "outputs": [],
      "source": [
        "s1 = (\"number\", \"three\")\n",
        "\n",
        "s2 = (\"starred\", \"in\")\n",
        "\n",
        "s3 = (\"actor\", \"in\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69ef66a2",
      "metadata": {
        "id": "69ef66a2"
      },
      "outputs": [],
      "source": [
        "print(top_k_best_candidates(lm, s1, 5, without=['<s>', '</s>']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee4e956",
      "metadata": {
        "id": "8ee4e956"
      },
      "source": [
        "### **Written 4.3.2** – Text Generation [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205129a7",
      "metadata": {
        "id": "205129a7"
      },
      "source": [
        "For this subtask, use the LM trained in Written 3.4.1\n",
        "\n",
        "Selecting words sequentially from a probability distribution is called _decoding_.\n",
        "\n",
        "Two popular decoding approaches are,\n",
        "1. **Max-probability decoding** - We consistently choose the candidate with maximum probability.\n",
        "2. **Random Sampling** - We sample a candidate randomly.\n",
        "2. **top-K Sampling** - We sample a candidate randomly from the top-K most probable choices.\n",
        "\n",
        "In this part, we will try the first two approaches to generate sentences.\n",
        "\n",
        "Q1. Use `generate_sentences()` method to generate sentences after the provided phrases from `s1` to `s3`. Use modes `random` and `max`. Report one of your favourite generations (for any strategy or phrase) along with its probability score.\n",
        "\n",
        "Q2. Which decoding strategy did you like better and why?"
      ]
    },
    {
      "cell_type": "raw",
      "id": "70302c60",
      "metadata": {
        "id": "70302c60"
      },
      "source": [
        "s1 = [\"<s>\", \"<s>\", \"number\", \"three\"]\n",
        "\n",
        "s2 = [\"<s>\", \"<s>\", \"starred\", \"in\"]\n",
        "\n",
        "s3 = [\"<s>\", \"<s>\", \"actor\", \"in\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcd3a291",
      "metadata": {
        "id": "dcd3a291"
      },
      "outputs": [],
      "source": [
        "# Random\n",
        "for _ in range(5):\n",
        "    print(list(generate_sentences_from_phrase(lm, 1, [\"<s>\", \"<s>\", \"number\", \"three\"], 0.2, mode=\"random\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d712619f",
      "metadata": {
        "id": "d712619f"
      },
      "source": [
        "**Aside (for fun!)**: Train your LM on Taylor Swift lyrics and generate the next hit!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b9047d",
      "metadata": {
        "id": "17b9047d"
      },
      "source": [
        "### **Written 4.4** – Battle of the LMs: GPT-2 vs Trigram [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d38359",
      "metadata": {
        "id": "41d38359"
      },
      "source": [
        "For this subtask, you will be generating text and comparing GPT-2 with your n-gram language model. \n",
        "\n",
        "Generative pretrained transformer (GPT) is a neural language model series created by OpenAI. The n-gram language model you trained has on average around 10K-20K parameters (`len(lm.model)`.) Compare that to the 175 billion parameters of the latest version of GPT-3!\n",
        "\n",
        "Let's see how GPT-2 compares to the LM you trained in Written 4.3.1 using `data/bbc/tech-small.txt` as the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e143e87c",
      "metadata": {
        "id": "e143e87c"
      },
      "outputs": [],
      "source": [
        "# Calculate your n-gram model's perplexity\n",
        "test = read_file(\"data/bbc/tech-small.txt\")\n",
        "test = preprocess(test,3)\n",
        "\n",
        "perplexity(lm,test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db542ceb",
      "metadata": {
        "id": "db542ceb"
      },
      "source": [
        "### Computing GPT-2's perplexity on test set\n",
        "\n",
        "You need to enable a GPU runtime from `Runtime` menu option. Go to `Runtime` → `Change Runtime Type` → `Hardware Accelerator (GPU)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6501dcd",
      "metadata": {
        "id": "d6501dcd"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ldf6ovg4B5Qc",
      "metadata": {
        "id": "Ldf6ovg4B5Qc"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "import torch\n",
        "\n",
        "model_id = \"distilgpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67zwTHSI0hCC",
      "metadata": {
        "id": "67zwTHSI0hCC"
      },
      "outputs": [],
      "source": [
        "test = read_file(\"data/bbc/tech-small.txt\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(test), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wmQKbMXjDFNj",
      "metadata": {
        "id": "wmQKbMXjDFNj"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "max_length = model.config.n_positions\n",
        "stride = 100\n",
        "\n",
        "nlls = []\n",
        "for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
        "    begin_loc = max(i + stride - max_length, 0)\n",
        "    end_loc = min(i + stride, encodings.input_ids.size(1))\n",
        "    trg_len = end_loc - i  # may be different from stride on last loop\n",
        "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "        neg_log_likelihood = outputs[0] * trg_len\n",
        "\n",
        "    nlls.append(neg_log_likelihood)\n",
        "\n",
        "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "giXGq0Z0DWdr",
      "metadata": {
        "id": "giXGq0Z0DWdr"
      },
      "outputs": [],
      "source": [
        "print(\"Perplexity using GPT2:\", ppl.item())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "abb966b7ea35b20666f3a996666172c7b349752d6f16e30915fecc2a1a4bf9ca"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}