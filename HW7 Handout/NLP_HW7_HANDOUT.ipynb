{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 11411/611 – NLP (Spring 23)\n",
        "## HW7 – Dependency Parsing\n",
        "\n",
        "Deadline: April 20, 2023 at 11:59pm EST\n"
      ],
      "metadata": {
        "id": "Fv-XPivVC5lB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency Trees are representations used for the syntactic analysis of sentences. To build a dependency tree, for each word we decide which other word \n",
        "it is a dependent of and what is the relationship they share.\n",
        "\n",
        "This is done by using the arc-standard system where each parse state is a configuration $\\mathcal{C} = \\{\\sigma, \\beta, \\alpha\\}$, in which $\\sigma$ is the stack of processed words, $\\beta$ is the buffer containing the remaining unprocessed words, and $\\alpha$ is a set of already performed actions. Such a parser has been described in lecture which included an example of a sentence being parsed using this system.\n",
        "\n",
        "Borrowing from a paper by Danqi Chen and Christopher D. Manning from Stanford, \n",
        "titled [\"A Fast and Accurate Dependency Parser using Neural Networks\"](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwiswpujhYL-AhVVVTUKHa3KB2QQFnoECBsQAQ&url=https%3A%2F%2Faclanthology.org%2FD14-1082.pdf&usg=AOvVaw2ORPPa4A6vPNJQIXMYmhyJ), we will be using features that describe the configuration at a given step to train a Neural Network to predict the next action.\n",
        "\n",
        "It may seem like there is a lot of information before your tasks actually begin, however, this is to clearly explain what the dependencies are doing to the data and to understand which aspect of dependency parsing are we really trying to implement. "
      ],
      "metadata": {
        "id": "celPenqFDuhI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf44eQVVVUdg",
        "outputId": "615b51ae-1066-494a-ee45-a1e487b90ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp_hw_dep'...\n",
            "remote: Enumerating objects: 188, done.\u001b[K\n",
            "remote: Counting objects: 100% (144/144), done.\u001b[K\n",
            "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
            "remote: Total 188 (delta 55), reused 107 (delta 45), pack-reused 44\u001b[K\n",
            "Receiving objects: 100% (188/188), 6.17 MiB | 4.13 MiB/s, done.\n",
            "Resolving deltas: 100% (70/70), done.\n",
            "/content/nlp_hw_dep\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting 2to3\n",
            "  Downloading 2to3-1.0-py3-none-any.whl (1.7 kB)\n",
            "Installing collected packages: 2to3\n",
            "Successfully installed 2to3-1.0\n",
            "RefactoringTool: Skipping optional fixer: buffer\n",
            "RefactoringTool: Skipping optional fixer: idioms\n",
            "RefactoringTool: Skipping optional fixer: set_literal\n",
            "RefactoringTool: Skipping optional fixer: ws_comma\n",
            "RefactoringTool: No changes to src/configuration.py\n",
            "RefactoringTool: No changes to src/decoder.py\n",
            "RefactoringTool: No changes to src/depModel.py\n",
            "RefactoringTool: Refactored src/eval.py\n",
            "--- src/eval.py\t(original)\n",
            "+++ src/eval.py\t(refactored)\n",
            "@@ -3,5 +3,5 @@\n",
            " \n",
            " uas,las = eval(os.path.abspath(sys.argv[1]), os.path.abspath(sys.argv[2]))\n",
            " \n",
            "-print 'Unlabeled attachment score', round(uas,2)\n",
            "-print 'Labeled attachment score', round(las,2)\n",
            "+print('Unlabeled attachment score', round(uas,2))\n",
            "+print('Labeled attachment score', round(las,2))\n",
            "RefactoringTool: Refactored src/evaluate.py\n",
            "--- src/evaluate.py\t(original)\n",
            "+++ src/evaluate.py\t(refactored)\n",
            "@@ -3,5 +3,5 @@\n",
            " \n",
            " uas, las = evaluate(os.path.abspath(sys.argv[1]), os.path.abspath(sys.argv[2]))\n",
            " \n",
            "-print(\"Unlabeled attachment score\", round(uas, 2))\n",
            "-print(\"Labeled attachment score\", round(las, 2))\n",
            "+print((\"Unlabeled attachment score\", round(uas, 2)))\n",
            "+print((\"Labeled attachment score\", round(las, 2)))\n",
            "RefactoringTool: No changes to src/gen.py\n",
            "RefactoringTool: No changes to src/gen_vocab.py\n",
            "RefactoringTool: No changes to src/unit_tests.py\n",
            "RefactoringTool: No changes to src/utils.py\n",
            "RefactoringTool: Files that were modified:\n",
            "RefactoringTool: src/configuration.py\n",
            "RefactoringTool: src/decoder.py\n",
            "RefactoringTool: src/depModel.py\n",
            "RefactoringTool: src/eval.py\n",
            "RefactoringTool: src/evaluate.py\n",
            "RefactoringTool: src/gen.py\n",
            "RefactoringTool: src/gen_vocab.py\n",
            "RefactoringTool: src/unit_tests.py\n",
            "RefactoringTool: src/utils.py\n"
          ]
        }
      ],
      "source": [
        "#@title Installing dependencies { display-mode: \"form\" }\n",
        "! git clone https://github.com/sparekh9/nlp_hw_dep\n",
        "%cd nlp_hw_dep/\n",
        "\n",
        "! pip install 2to3\n",
        "%mkdir data outputs\n",
        "! 2to3 --write --nobackups -x import src/*.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "pSf3xxxGIlsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "YLPF8C6yHeBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UD Treebank is a collection of sentences which have been parsed by humans. We will be using the [GUM treebank](https://gucorpling.org/gum/annotations.html) to extract training data. Specifically, GUM incorporated parsing using the package Stanza which was then corrected by human collaborators.\n",
        "\n",
        "The treebank is formatted as `.conll` files which are the standard for parsing. Feel free to look through any of the files available [here](https://github.com/amir-zeldes/gum/tree/master/dep) in the GitHub repository for the GUM treebank. \n",
        "These files specify information about the sentence, like the words, stems of the words, POS-tags, the pinnacle of the word (what the words is a dependent of i.e. where the arc begins from) and the nature of the dependence.\n",
        "\n",
        "There are variety of topics which the sentences have been categorized into. We have taken an 80-20 split over all of these topics to create the `train.conll` and `dev.conll` files.\n",
        "\n",
        "\n",
        "\n",
        "We see that this data needs to be transformed into configurations i.e. usable training data. The code provided parses the sentence using the given information in the `.conll` files\n",
        "and captures the configuration of the parser at each step producing the necessary data needed to train our model, including the gold action that was performed - SHIFT, or one of LEFT-ARC, or RIGHT-ARC with the corresponding dependency. We will be using 52 features - 20 types of word features, 20 types of POS features, and 12 types of dependency features bring our total to 52 features. These would our input vectors i.e. our `X` and the gold action would be the output i.e. the `y` we are trying to predict.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fAUdLkT_HKyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trees to Instances (Data Generation)\n",
        "! python src/gen.py trees/train.conll data/train.data\n",
        "! python src/gen.py trees/dev.conll data/dev.data"
      ],
      "metadata": {
        "id": "rsu_un89HJEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e852ca4-544c-42f4-ec8a-01f3fb9b3dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlp_hw_dep/src/configuration.py:128: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  label_feats.append(self.arcs[f][1]) if f and self.arcs[f][\n",
            "100...200...300...400...500...600...700...800...900...1000...1100...1200...1300...1400...1500...1600...1700...1800...1900...2000...2100...2200...2300...2400...2500...2600...2700...2800...2900...3000...3100...3200...3300...3400...3500...3600...3700...3800...3900...4000...4100...4200...4300...4400...4500...4600...4700...4800...4900...5000...5100...5200...5300...5400...5500...5600...5700...5800...5900...6000...6100...6200...6300...6400...6500...6600...6700...6800...6900...7000...7100...7200...7300...7400...7500...7600...7700...7800...7900...8000...8100...8200...8300...done!\n",
            "100...200...300...400...500...600...700...800...900...1000...1100...1200...1300...1400...1500...1600...1700...1800...1900...2000...2100...2200...2300...2400...2500...2600...2700...2800...2900...3000...3100...3200...done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following line of code generates integer mappings of words, POS-tags, dependency labels, and actions of a given configuration of the parser which will be used to train the model. This is necessary since the data is currently in the form of strings which cannot directly be fed into a model. These will stored in the `data` folder as `vocabs.-` files."
      ],
      "metadata": {
        "id": "iqjc_cDPFbc3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27ys9op-3Pv-"
      },
      "outputs": [],
      "source": [
        "# Vocab Creation\n",
        "! python src/gen_vocab.py trees/train.conll data/vocabs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use these mappings to convert the string for a configuration into vectors of integers by defining a class DataSamples inherited from the PyTorch class \n",
        "`DataSet`. You may look through the code by double-clicking the following cell."
      ],
      "metadata": {
        "id": "aGOfvysfWG0M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SI-i5-ebp4V",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Converting configuration strings into integer vectors\n",
        "# Data Loader\n",
        "def remap_vocab(vocab, null_tok=\"<null>\"):\n",
        "    # we want to map null/unk to 0\n",
        "    id2vocab = [null_tok] + [v for v in vocab if v != null_tok]\n",
        "    vocab = {v: i for i, v in enumerate(id2vocab)}\n",
        "    return vocab\n",
        "\n",
        "def print_dict(d):\n",
        "  for i, e in enumerate(d):\n",
        "    if i == 0:\n",
        "      print(\"{  '\" + e + \"': \" + str(d[e]) + \",\")\n",
        "    else:\n",
        "      print(\"   '\" + e + \"': \" + str(d[e]) + \",\")\n",
        "    if i == 5: break\n",
        "  print(\"   ...\")\n",
        "  print(\"}\")\n",
        "\n",
        "class DataSamples(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, partition=\"train\"):\n",
        "        self.data_path = data_path\n",
        "        self.partition = partition\n",
        "        self.create_dictionaries()\n",
        "        with open(self.data_path + partition + \".data\", \"r\") as f:\n",
        "            self.raw_data = f.readlines()\n",
        "        self.data = [a.split(\" \") for a in self.raw_data]\n",
        "        self.create_vectors()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        return torch.from_numpy(self.data[ind])\n",
        "\n",
        "    # generating 53-length vector for a given configuration\n",
        "    def create_vectors(self):\n",
        "        abset_pos_count = 0\n",
        "        abset_poss = set()\n",
        "        abset_label_count = 0\n",
        "        abset_labels = set()\n",
        "        abset_action_count = 0\n",
        "        abset_actions = set()\n",
        "\n",
        "        for ind, inst in enumerate(self.data):\n",
        "            for i in range(len(inst)):\n",
        "                if i < 20:\n",
        "                    if inst[i] in self.word_dict:\n",
        "                        inst[i] = self.word_dict[inst[i]]\n",
        "                    else:\n",
        "                        inst[i] = self.word_dict[\"<unk>\"]\n",
        "                elif 20 <= i < 40:\n",
        "                    if inst[i] in self.pos_dict:\n",
        "                        inst[i] = self.pos_dict[inst[i]]\n",
        "                    else:\n",
        "                        abset_pos_count += 1\n",
        "                        abset_poss.add(inst[i])\n",
        "                        inst[i] = self.pos_dict[\"<null>\"]\n",
        "                elif 40 <= i < 52:\n",
        "                    if inst[i] in self.label_dict:\n",
        "                        inst[i] = self.label_dict[inst[i]]\n",
        "                    else:\n",
        "                        abset_label_count += 1\n",
        "                        abset_labels.add(inst[i])\n",
        "                        inst[i] = self.label_dict[\"<null>\"]\n",
        "                else:\n",
        "                    if inst[i] in self.action_dict:\n",
        "                        inst[i] = self.action_dict[inst[i]]\n",
        "                    elif inst[i][:-1] in self.action_dict:\n",
        "                        inst[i] = self.action_dict[inst[i][:-1]]\n",
        "                    else:\n",
        "                        abset_action_count += 1\n",
        "                        abset_actions.add(inst[i][:-1])\n",
        "                        inst[i] = self.action_dict[\n",
        "                            inst[i][:-1].split(\":\")[0] + \":<null>\"\n",
        "                        ]\n",
        "        self.data = np.array(self.data)\n",
        "\n",
        "    def create_dictionaries(self):\n",
        "        with open(self.data_path + \"vocabs.word\", \"r\") as f:\n",
        "            words = f.readlines()\n",
        "        words = [a.split(\" \") for a in words]\n",
        "\n",
        "        with open(self.data_path + \"vocabs.pos\", \"r\") as f:\n",
        "            poss = f.readlines()\n",
        "        poss = [a.split(\" \") for a in poss]\n",
        "\n",
        "        with open(self.data_path + \"vocabs.labels\", \"r\") as f:\n",
        "            labels = f.readlines()\n",
        "        labels = [a.split(\" \") for a in labels]\n",
        "\n",
        "        with open(self.data_path + \"vocabs.actions\", \"r\") as f:\n",
        "            actions = f.readlines()\n",
        "        actions = [a.split(\" \") for a in actions]\n",
        "\n",
        "        self.word_dict = remap_vocab({a[0]: int(a[1]) for a in words}, \"<unk>\")\n",
        "        self.pos_dict = remap_vocab({a[0]: int(a[1]) for a in poss}, \"<null>\")\n",
        "        self.label_dict = remap_vocab({a[0]: int(a[1]) for a in labels}, \"<null>\")\n",
        "        self.action_dict = {a[0]: int(a[1]) for a in actions}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k092DmHfbEu"
      },
      "outputs": [],
      "source": [
        "train = DataSamples(\"data/\", partition=\"train\")\n",
        "dev = DataSamples(\"data/\", partition=\"dev\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running this, we have 4 dictionaries that contain the integer mappings. These can be accessed through `train.word_dict`, `train.pos_dict`, `train.label_dict`, and `train.action_dict`. Have a look at what some of the elements in these look like.\n",
        "\n"
      ],
      "metadata": {
        "id": "1fpVte0gbgdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dictionary contains maps all the words present in all the sentences in the original dataset."
      ],
      "metadata": {
        "id": "IUc7iMlyhEZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_dict(train.word_dict)"
      ],
      "metadata": {
        "id": "LfRy_zQOXvop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed2a4669-c6a2-401c-cfbe-e811a19e758c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{  '<unk>': 0,\n",
            "   '<null>': 1,\n",
            "   '<root>': 2,\n",
            "   '1': 3,\n",
            "   'Introduction': 4,\n",
            "   'and': 5,\n",
            "   ...\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dictionary contains maps all the POS-tags present in all the sentences in the original dataset."
      ],
      "metadata": {
        "id": "PFNzfRbwhML3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_dict(train.pos_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXdupsAGgXHc",
        "outputId": "bb9bbd2d-c5ba-485e-d7b5-077150790006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{  '<null>': 0,\n",
            "   'ADJ': 1,\n",
            "   'SYM': 2,\n",
            "   'VERB': 3,\n",
            "   'PART': 4,\n",
            "   'AUX': 5,\n",
            "   ...\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dictionary contains maps all the dependency labels present in the original dataset."
      ],
      "metadata": {
        "id": "N0Pq-mBCik4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_dict(train.label_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCDOlEdqggj1",
        "outputId": "d621fe36-8a5e-4e31-cf52-1ba48ee20f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{  '<null>': 0,\n",
            "   'obl:agent': 1,\n",
            "   'det': 2,\n",
            "   'advmod': 3,\n",
            "   'reparandum': 4,\n",
            "   'punct': 5,\n",
            "   ...\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dictionary contains maps all the GOLD-STANDARD actions present in the original dataset."
      ],
      "metadata": {
        "id": "lei2xmF6jIF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_dict(train.action_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhhhQLmpgnP_",
        "outputId": "8d258082-59aa-4da0-8132-c8ee0e4ca4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{  'SHIFT': 0,\n",
            "   'LEFT-ARC:obl:agent': 1,\n",
            "   'LEFT-ARC:det': 2,\n",
            "   'LEFT-ARC:advmod': 3,\n",
            "   'LEFT-ARC:reparandum': 4,\n",
            "   'LEFT-ARC:punct': 5,\n",
            "   ...\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A key feature of the `Dataset` class is that we specify a function `__getitem__(self, ind)` which gets the item at that given index. These can be accessed by simply treating the object as an Iterable i.e. by indexing into it. Here's an example of what an element in train looks like. The length should match be equal to our expected length for `X` plus our `y`.\n"
      ],
      "metadata": {
        "id": "6d_JpWQaJjh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRD7zHOBKDyK",
        "outputId": "aff37e3d-d297-415e-cbaf-2cfa65b98ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2,  1,  1,  1,  3,  4,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "         1,  1, 18,  0,  0,  0, 14, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert(len(train[0]) == 53)"
      ],
      "metadata": {
        "id": "MX-qBMYQK2aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the final step, we convert the train and dev `DataSample` objects to a PyTorch `DataLoader` in order to split the data into batches to be used while training the model."
      ],
      "metadata": {
        "id": "l8mbKtJljO7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9l8fcyJ2U2o"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train, batch_size=1024, shuffle=True, num_workers=0)\n",
        "dev_loader = DataLoader(dev, batch_size=1024, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure that you are using the GPU and `device=cuda` when you run this cell."
      ],
      "metadata": {
        "id": "tlRDeAq3nBNI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsWpPPosyAJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188049a7-9fa8-494b-d6f2-9caabce065ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"PyTorch device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1 - Defining the Classifier"
      ],
      "metadata": {
        "id": "zYP1LCi4aSAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will be defining the classifier using the `nn.Module` class from PyTorch.\n",
        "\n",
        "As explained in the handout, we will be using an Embedding layer which will be separated into 3 parts. The output of these will be passed into the hidden layers you define using the `layer_sizes` variable. \n",
        "\n",
        "Note that traditionally for classification problems we would apply `Softmax` or `Sigmoid` to the output layer. However, we won't be working with the probabilities and the raw, unormalized values are enough to simply obtain the best action which is what we will be using to parse the sentence. \n",
        "\n",
        "All the necessary layers can be obtained using the `nn` module which has already been imported. For further information, you may look through the [documentation](https://pytorch.org/docs/stable/nn.html).\n"
      ],
      "metadata": {
        "id": "LOaJ-pnsvBdU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzNbo5bf9QXo"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        word_vocab_size,\n",
        "        word_emb_size,\n",
        "        pos_vocab_size,\n",
        "        pos_emb_size,\n",
        "        depl_vocab_size,\n",
        "        depl_emb_size,\n",
        "        out_size,\n",
        "        layer_sizes,\n",
        "        dropout,\n",
        "    ):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        # TODO: Define the embedding layers according to the relevant vocab_size and embedding size\n",
        "        self.word_emb = \n",
        "        self.pos_emb = \n",
        "        self.depl_emb = \n",
        "\n",
        "        # TODO: Determine the input size to the hidden layers using the formula provided in the handout\n",
        "        in_size = \n",
        "\n",
        "        input_sizes = [in_size] + layer_sizes[:-1]\n",
        "        output_sizes = layer_sizes\n",
        "        layers = []\n",
        "\n",
        "        # Adding the hidden layers\n",
        "        for s1, s2 in zip(input_sizes, output_sizes):\n",
        "\n",
        "            # TODO: Append a linear layer to layers with the input and output size\n",
        "            \n",
        "\n",
        "            # TODO: Append a LeakyReLu layer, feel free to experiment here\n",
        "            \n",
        "\n",
        "            # TODO: Append a dropout layer with our input dropout value\n",
        "            \n",
        "\n",
        "        # TODO: Append the output layer using the layer size of the last year \n",
        "        # as the input and out_size as the output\n",
        "        \n",
        "\n",
        "        # TODO: Initialize the layers of the networks by passing the \n",
        "        # unpacked list through a PyTorch Sequential container\n",
        "        self.layers = \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # TODO: Define the inputs to the each embedding layer\n",
        "        # Hint: Each element in input has length 52, to be separated into three parts\n",
        "        word_emb_input = \n",
        "        pos_emb_input = \n",
        "        depl_emb_input = \n",
        "\n",
        "        # Here we pass the inputs through the embedding layers and concatenate their outputs.\n",
        "        embs = [\n",
        "            *self.word_emb(word_emb_input).split(1, dim=1),\n",
        "            *self.pos_emb(pos_emb_input).split(1, dim=1),\n",
        "            *self.depl_emb(depl_emb_input).split(1, dim=1),\n",
        "        ]\n",
        "        embs = [torch.squeeze(tens) for tens in embs]\n",
        "        embs = torch.cat(embs, dim=-1)\n",
        "\n",
        "        # TODO: Pass the output of the embeddings layer through the rest of the layers.\n",
        "        output = \n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing your definition of the `Classifier` class"
      ],
      "metadata": {
        "id": "rqj7FdHKZCgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define a dummy classifier, and perform a singular forward pass to confirm whether the layers \n",
        "# have been added correctly\n",
        "dummy_in = torch.randint(1,10,(1024,52))\n",
        "dummy_classifier = Classifier(7421,64,19,32,54,32,109, [2048, 512, 256], 0.3)\n",
        "dummy_out = dummy_classifier.forward(dummy_in)\n",
        "\n",
        "assert(dummy_out.shape == (1024,109))"
      ],
      "metadata": {
        "id": "0BBGG01AY5rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once, you pass this test case, copy the code into the `classifier.py` file in the handout and make a preliminary submission to Gradescope."
      ],
      "metadata": {
        "id": "E5OCm6oTiqNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2 - Initializing the model"
      ],
      "metadata": {
        "id": "NbHTDmV1O9nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having defined the Classifier class, we will use this to create a model of our own. \n",
        "\n",
        "While initializing the model, the following parameters should be initialized according to the lengths of the dictionaries that contain the vocabularies i.e. number of unique words, POS-tags, dependency labels, and finally actions:\n",
        "\n",
        "*   `word_vocab_size`\n",
        "*   `pos_vocab_size`\n",
        "*   `depl_vocab_size`\n",
        "*   `out_size`\n",
        "\n",
        "Think about what the `out_size` corresponds to and define it accordingly.\n",
        "\n",
        "The following remaining variables may be considered hyperparameters to be tuned by you, though we have some suggestions:\n",
        "\n",
        "* `word_emb_size` : < 100\n",
        "* `pos_emb_size` : < 100\n",
        "* `depl_emb_size` : < 100\n",
        "* `layer_sizes` - 3 layers are sufficient to achieve the benchmark accuracy, too many may result in overfitting, populate the list with the number of neurons, in order.\n",
        "* `dropout` : < 0.5\n",
        "\n",
        "The same can be said for the learning rate and the number of `epochs`. We'd suggest keeping the `epochs` < 50, again to prevent overfitting. For the learning rate, too high a value and the model won't be able to reach optimal performance, too low a value and the model won't train quick enough. We'd suggest a value between 0.0005 and 0.05."
      ],
      "metadata": {
        "id": "8GZ_enVDQvQg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiIbRcIrynRQ"
      },
      "outputs": [],
      "source": [
        "# TODO: Initialize the model, read the above instructions before beginning\n",
        "model = Classifier(\n",
        "    word_vocab_size = ,\n",
        "    word_emb_size = ,\n",
        "    pos_vocab_size = ,\n",
        "    pos_emb_size = ,\n",
        "    depl_vocab_size = ,\n",
        "    depl_emb_size = ,\n",
        "    out_size = ,\n",
        "    layer_sizes = [],\n",
        "    dropout =\n",
        ")\n",
        "\n",
        "# TODO: Initialize an AdamW optimizer with the model parameters and an appropriate learning rate\n",
        "optimizer = \n",
        "\n",
        "# TODO: Initialize the loss function to cross-entropy loss\n",
        "criterion =\n",
        "\n",
        "# We define a scheduler to decay the learning rate as we move through the epochs\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.3)\n",
        "\n",
        "# TODO: Initialize the number of epochs\n",
        "epochs = \n",
        "\n",
        "# Setting the model to run and train on the GPU\n",
        "model.to(device)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 3 - Training the Model"
      ],
      "metadata": {
        "id": "YrYvpQvyU29c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having initialized the model with its parameters, we will train it on the training data using the `train_loader (DataLoader)` object. This is a typical NN training loop which you have likely seen before.\n",
        "\n",
        "Your model should reach 0.9 accuracy fairly quickly and this will likely be necessary in order to obtain the desired performance while parsing.\n",
        "\n",
        "You may refer to the Train the Network section of [this](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) tutorial."
      ],
      "metadata": {
        "id": "1oPkiMapW1Bg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxGYKs6h2wCR"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    for idx, data_instance in enumerate(train_loader):\n",
        "\n",
        "        # TODO: Extract the inputs and labels from the data_instance\n",
        "        # and set them to the device \n",
        "        # Hint: Data Instance as the shape (1024, 53)\n",
        "        inputs = \n",
        "        labels = \n",
        "\n",
        "        # TODO: Pass the inputs to the model to get the output\n",
        "        output = \n",
        "\n",
        "        # TODO: Calculate the loss by using the loss function \n",
        "        # defined earlier, passing in outputs and labels.\n",
        "        # Remember: To convert labels to an int64 tensor\n",
        "        loss = \n",
        "\n",
        "        # TODO: Reset the optimizer for each batch\n",
        "        \n",
        "\n",
        "        # TODO: Backward propagate through the loss function\n",
        "        \n",
        "\n",
        "        # TODO: Step through the optimizer\n",
        "        \n",
        "\n",
        "        # Calculating the training accuracy\n",
        "        total_loss += loss.item()\n",
        "        correct = (labels == torch.argmax(output, dim=1)).float().sum()\n",
        "        total_acc += correct\n",
        "    accuracy = total_acc / len(train)\n",
        "    print(\n",
        "        \"Epoch Number: {}, Loss: {}, Accuracy: {}\".format(epoch, total_loss, accuracy)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation on the Dev Set"
      ],
      "metadata": {
        "id": "H9E8pQT4XA6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The naive process to evaulate the model would be to determine the accuracy of the model on predicting the actions for the dev set. While this is a good starting metric, for parsing we prefer to use the Labelled Attachment Score which determines whether a sentences was parsed correctly instead of whether an action was predicted correctly."
      ],
      "metadata": {
        "id": "AQ_nioIGXWnB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQUODeGdphuq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Testing the accuracy of predicting gold actions on the dev set\n",
        "\n",
        "correct = 0\n",
        "for data_instance in dev_loader:\n",
        "    x = data_instance[:, :-1].to(device)\n",
        "    y = data_instance[:, -1].to(device)\n",
        "    y_pred = torch.argmax(model(x), dim=1)\n",
        "    correct += (y == y_pred).cpu().numpy().sum()\n",
        "dev_accuracy = (correct / len(dev))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert(dev_accuracy >= 0.80)"
      ],
      "metadata": {
        "id": "WDKeJFpGUM1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying the paths of the input dev.conll files and your output file."
      ],
      "metadata": {
        "id": "sKBdqhEcYEyk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf_ZkD96i6L7"
      },
      "outputs": [],
      "source": [
        "input_p = os.path.abspath(\"trees/dev.conll\")\n",
        "output_p = os.path.abspath(\"outputs/dev.out\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAOkdpufi6L7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Calculating the Labelled Attachment Score\n",
        "def get_score_func(model, train_data, device):\n",
        "    def _score_fn(inputs):\n",
        "        nonlocal model, train_data, device\n",
        "        words = [train_data.word_dict[w] if w in train_data.word_dict else 0 for w in inputs[:20]]\n",
        "        pos = [train_data.pos_dict[w] if w in train_data.pos_dict else 0 for w in inputs[20:40]]\n",
        "        labels = [train_data.label_dict[w] if w in train_data.label_dict else 0 for w in inputs[40:]]\n",
        "        ipt = torch.tensor([[*words, *pos, *labels]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            preds = model(ipt).cpu().numpy().squeeze()\n",
        "        return preds\n",
        "    return _score_fn\n",
        "\n",
        "score_fn = get_score_func(model, train, device)\n",
        "actions_map = {i:act for act,i in train.action_dict.items()}\n",
        "\n",
        "%cd src\n",
        "from decoder import Decoder\n",
        "Decoder(score_fn, actions_map).parse(input_p, output_p)\n",
        "\n",
        "from utils import evaluate\n",
        "_ , las = evaluate(input_p, output_p)\n",
        "%cd ..\n",
        "print(\"Labeled attachment score\", round(las, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOBhMJvYi6L8"
      },
      "outputs": [],
      "source": [
        "assert(las >= 0.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the `dev.out` file from the outputs folder and submit it along with the `classifier.py` file from Task 1."
      ],
      "metadata": {
        "id": "Ir7AIzt-YWEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would like to acknowledge our use of the Dependency Parsing with\n",
        "Feed-Forward Neural Network HW from Columbia University by Prof. Michael Collins in the design of this homework."
      ],
      "metadata": {
        "id": "SeJPJsY8rJnM"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}